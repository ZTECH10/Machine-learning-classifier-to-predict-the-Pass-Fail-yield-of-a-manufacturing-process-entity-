{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "37puETfgRzzg"
   },
   "source": [
    "# Precision Manufacturing: Harnessing Machine Learning for Pass-Fail Yield Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Description\n",
    "Precision Manufacturing: Harnessing Machine Learning for Pass-Fail Yield Prediction\n",
    "\n",
    "\n",
    "\n",
    "**Project Overview**\n",
    "\n",
    "üî¨üë®‚Äçüî¨ A complex modern semiconductor manufacturing process is normally under constant surveillance via the monitoring of signals/variables collected from sensors and or process measurement points. üìä\n",
    "\n",
    "However, not all of these signals are equally valuable in a specific monitoring system. The measured signals contain a combination of useful information, irrelevant information as well as noise. üìâüìù‚ùå\n",
    "\n",
    "Engineers typically have a much larger number of signals than are actually required. If we consider each type of signal as a feature, then feature selection may be applied to identify the most relevant signals. üõ†Ô∏èüìà\n",
    "\n",
    "The Process Engineers may then use these signals to determine key factors contributing to yield excursions downstream in the process. This will enable an increase in process throughput, decreased time to learning, and reduce per-unit production costs. üíπ‚è≥üí≤\n",
    "\n",
    "These signals can be used as features to predict the yield type. And by analyzing and trying out different combinations of features, essential signals that are impacting the yield type can be identified. üìäüîÑüí°\n",
    "\n",
    "**Data preprocessing:**\n",
    "- ‚ú® Handle missing values\n",
    "\n",
    "- üìä Encoding the Output Data\n",
    "\n",
    "- üßê Checking if the dataset contains any NULL values\n",
    "\n",
    "- üìäüßê Taking care of outliers\n",
    "\n",
    "- üìè Feature Scaling\n",
    "\n",
    "**Data Visualization:**\n",
    "\n",
    "- üìùüîç Reviewing the data for some general information\n",
    "\n",
    "\n",
    "**Model Training:**\n",
    "\n",
    "- üß© Split data into training and testing sets\n",
    "\n",
    "- üöÄ Train selected models on training data\n",
    "\n",
    "**ML Models:**\n",
    "\n",
    "- üìä Logistic regression\n",
    "\n",
    "- üß† ANN\n",
    "\n",
    "**Model Evaluation:**\n",
    "\n",
    "- üìä Evaluate model performance using accuracy metrics\n",
    "\n",
    "- üìâ Analyze confusion matrix\n",
    "\n",
    "- üìà Plotting the ANN Learning Curve\n",
    "\n",
    "**Tuning:**\n",
    "\n",
    "- ‚öôÔ∏è Hyperparameter tuning for model optimization\n",
    "\n",
    "\n",
    "Dataset: SemiconductorManufacturingProcessDataset.csv\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EoRP98MpR-qj"
   },
   "source": [
    "## Importing the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N-qiINBQSK2g"
   },
   "outputs": [],
   "source": [
    "#For basic operations\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "#For visulauzations\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "\n",
    "\n",
    "#Tensorflow\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RopL7tUZSQkT"
   },
   "source": [
    "## Importing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WwEPNDWySTKm"
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('SemiconductorManufacturingProcessDataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Showing the Dataset in a Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>Sensor 1</th>\n",
       "      <th>Sensor 2</th>\n",
       "      <th>Sensor 3</th>\n",
       "      <th>Sensor 4</th>\n",
       "      <th>Sensor 5</th>\n",
       "      <th>Sensor 6</th>\n",
       "      <th>Sensor 7</th>\n",
       "      <th>Sensor 8</th>\n",
       "      <th>Sensor 9</th>\n",
       "      <th>...</th>\n",
       "      <th>Sensor 429</th>\n",
       "      <th>Sensor 430</th>\n",
       "      <th>Sensor 431</th>\n",
       "      <th>Sensor 432</th>\n",
       "      <th>Sensor 433</th>\n",
       "      <th>Sensor 434</th>\n",
       "      <th>Sensor 435</th>\n",
       "      <th>Sensor 436</th>\n",
       "      <th>Sensor 437</th>\n",
       "      <th>Pass/Fail</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7/19/2008 11:55</td>\n",
       "      <td>3030.93</td>\n",
       "      <td>2564.00</td>\n",
       "      <td>2187.7333</td>\n",
       "      <td>1411.1265</td>\n",
       "      <td>1.3602</td>\n",
       "      <td>97.6133</td>\n",
       "      <td>0.1242</td>\n",
       "      <td>1.5005</td>\n",
       "      <td>0.0162</td>\n",
       "      <td>...</td>\n",
       "      <td>14.9509</td>\n",
       "      <td>0.5005</td>\n",
       "      <td>0.0118</td>\n",
       "      <td>0.0035</td>\n",
       "      <td>2.3630</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7/19/2008 12:32</td>\n",
       "      <td>3095.78</td>\n",
       "      <td>2465.14</td>\n",
       "      <td>2230.4222</td>\n",
       "      <td>1463.6606</td>\n",
       "      <td>0.8294</td>\n",
       "      <td>102.3433</td>\n",
       "      <td>0.1247</td>\n",
       "      <td>1.4966</td>\n",
       "      <td>-0.0005</td>\n",
       "      <td>...</td>\n",
       "      <td>10.9003</td>\n",
       "      <td>0.5019</td>\n",
       "      <td>0.0223</td>\n",
       "      <td>0.0055</td>\n",
       "      <td>4.4447</td>\n",
       "      <td>0.0096</td>\n",
       "      <td>0.0201</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>208.2045</td>\n",
       "      <td>Pass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7/19/2008 13:17</td>\n",
       "      <td>2932.61</td>\n",
       "      <td>2559.94</td>\n",
       "      <td>2186.4111</td>\n",
       "      <td>1698.0172</td>\n",
       "      <td>1.5102</td>\n",
       "      <td>95.4878</td>\n",
       "      <td>0.1241</td>\n",
       "      <td>1.4436</td>\n",
       "      <td>0.0041</td>\n",
       "      <td>...</td>\n",
       "      <td>9.2721</td>\n",
       "      <td>0.4958</td>\n",
       "      <td>0.0157</td>\n",
       "      <td>0.0039</td>\n",
       "      <td>3.1745</td>\n",
       "      <td>0.0584</td>\n",
       "      <td>0.0484</td>\n",
       "      <td>0.0148</td>\n",
       "      <td>82.8602</td>\n",
       "      <td>Fail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7/19/2008 14:43</td>\n",
       "      <td>2988.72</td>\n",
       "      <td>2479.90</td>\n",
       "      <td>2199.0333</td>\n",
       "      <td>909.7926</td>\n",
       "      <td>1.3204</td>\n",
       "      <td>104.2367</td>\n",
       "      <td>0.1217</td>\n",
       "      <td>1.4882</td>\n",
       "      <td>-0.0124</td>\n",
       "      <td>...</td>\n",
       "      <td>8.5831</td>\n",
       "      <td>0.4990</td>\n",
       "      <td>0.0103</td>\n",
       "      <td>0.0025</td>\n",
       "      <td>2.0544</td>\n",
       "      <td>0.0202</td>\n",
       "      <td>0.0149</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>73.8432</td>\n",
       "      <td>Pass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7/19/2008 15:22</td>\n",
       "      <td>3032.24</td>\n",
       "      <td>2502.87</td>\n",
       "      <td>2233.3667</td>\n",
       "      <td>1326.5200</td>\n",
       "      <td>1.5334</td>\n",
       "      <td>100.3967</td>\n",
       "      <td>0.1235</td>\n",
       "      <td>1.5031</td>\n",
       "      <td>-0.0031</td>\n",
       "      <td>...</td>\n",
       "      <td>10.9698</td>\n",
       "      <td>0.4800</td>\n",
       "      <td>0.4766</td>\n",
       "      <td>0.1045</td>\n",
       "      <td>99.3032</td>\n",
       "      <td>0.0202</td>\n",
       "      <td>0.0149</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>73.8432</td>\n",
       "      <td>Pass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1562</th>\n",
       "      <td>10/16/2008 15:13</td>\n",
       "      <td>2899.41</td>\n",
       "      <td>2464.36</td>\n",
       "      <td>2179.7333</td>\n",
       "      <td>3085.3781</td>\n",
       "      <td>1.4843</td>\n",
       "      <td>82.2467</td>\n",
       "      <td>0.1248</td>\n",
       "      <td>1.3424</td>\n",
       "      <td>-0.0045</td>\n",
       "      <td>...</td>\n",
       "      <td>11.7256</td>\n",
       "      <td>0.4988</td>\n",
       "      <td>0.0143</td>\n",
       "      <td>0.0039</td>\n",
       "      <td>2.8669</td>\n",
       "      <td>0.0068</td>\n",
       "      <td>0.0138</td>\n",
       "      <td>0.0047</td>\n",
       "      <td>203.1720</td>\n",
       "      <td>Pass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1563</th>\n",
       "      <td>10/16/2008 20:49</td>\n",
       "      <td>3052.31</td>\n",
       "      <td>2522.55</td>\n",
       "      <td>2198.5667</td>\n",
       "      <td>1124.6595</td>\n",
       "      <td>0.8763</td>\n",
       "      <td>98.4689</td>\n",
       "      <td>0.1205</td>\n",
       "      <td>1.4333</td>\n",
       "      <td>-0.0061</td>\n",
       "      <td>...</td>\n",
       "      <td>17.8379</td>\n",
       "      <td>0.4975</td>\n",
       "      <td>0.0131</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>2.6238</td>\n",
       "      <td>0.0068</td>\n",
       "      <td>0.0138</td>\n",
       "      <td>0.0047</td>\n",
       "      <td>203.1720</td>\n",
       "      <td>Pass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1564</th>\n",
       "      <td>10/17/2008 5:26</td>\n",
       "      <td>2978.81</td>\n",
       "      <td>2379.78</td>\n",
       "      <td>2206.3000</td>\n",
       "      <td>1110.4967</td>\n",
       "      <td>0.8236</td>\n",
       "      <td>99.4122</td>\n",
       "      <td>0.1208</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>17.7267</td>\n",
       "      <td>0.4987</td>\n",
       "      <td>0.0153</td>\n",
       "      <td>0.0041</td>\n",
       "      <td>3.0590</td>\n",
       "      <td>0.0197</td>\n",
       "      <td>0.0086</td>\n",
       "      <td>0.0025</td>\n",
       "      <td>43.5231</td>\n",
       "      <td>Pass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1565</th>\n",
       "      <td>10/17/2008 6:01</td>\n",
       "      <td>2894.92</td>\n",
       "      <td>2532.01</td>\n",
       "      <td>2177.0333</td>\n",
       "      <td>1183.7287</td>\n",
       "      <td>1.5726</td>\n",
       "      <td>98.7978</td>\n",
       "      <td>0.1213</td>\n",
       "      <td>1.4622</td>\n",
       "      <td>-0.0072</td>\n",
       "      <td>...</td>\n",
       "      <td>19.2104</td>\n",
       "      <td>0.5004</td>\n",
       "      <td>0.0178</td>\n",
       "      <td>0.0038</td>\n",
       "      <td>3.5662</td>\n",
       "      <td>0.0262</td>\n",
       "      <td>0.0245</td>\n",
       "      <td>0.0075</td>\n",
       "      <td>93.4941</td>\n",
       "      <td>Pass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1566</th>\n",
       "      <td>10/17/2008 6:07</td>\n",
       "      <td>2944.92</td>\n",
       "      <td>2450.76</td>\n",
       "      <td>2195.4444</td>\n",
       "      <td>2914.1792</td>\n",
       "      <td>1.5978</td>\n",
       "      <td>85.1011</td>\n",
       "      <td>0.1235</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>22.9183</td>\n",
       "      <td>0.4987</td>\n",
       "      <td>0.0181</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>3.6275</td>\n",
       "      <td>0.0117</td>\n",
       "      <td>0.0162</td>\n",
       "      <td>0.0045</td>\n",
       "      <td>137.7844</td>\n",
       "      <td>Pass</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1567 rows √ó 439 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Time  Sensor 1  Sensor 2   Sensor 3   Sensor 4  Sensor 5  \\\n",
       "0      7/19/2008 11:55   3030.93   2564.00  2187.7333  1411.1265    1.3602   \n",
       "1      7/19/2008 12:32   3095.78   2465.14  2230.4222  1463.6606    0.8294   \n",
       "2      7/19/2008 13:17   2932.61   2559.94  2186.4111  1698.0172    1.5102   \n",
       "3      7/19/2008 14:43   2988.72   2479.90  2199.0333   909.7926    1.3204   \n",
       "4      7/19/2008 15:22   3032.24   2502.87  2233.3667  1326.5200    1.5334   \n",
       "...                ...       ...       ...        ...        ...       ...   \n",
       "1562  10/16/2008 15:13   2899.41   2464.36  2179.7333  3085.3781    1.4843   \n",
       "1563  10/16/2008 20:49   3052.31   2522.55  2198.5667  1124.6595    0.8763   \n",
       "1564   10/17/2008 5:26   2978.81   2379.78  2206.3000  1110.4967    0.8236   \n",
       "1565   10/17/2008 6:01   2894.92   2532.01  2177.0333  1183.7287    1.5726   \n",
       "1566   10/17/2008 6:07   2944.92   2450.76  2195.4444  2914.1792    1.5978   \n",
       "\n",
       "      Sensor 6  Sensor 7  Sensor 8  Sensor 9  ...  Sensor 429  Sensor 430  \\\n",
       "0      97.6133    0.1242    1.5005    0.0162  ...     14.9509      0.5005   \n",
       "1     102.3433    0.1247    1.4966   -0.0005  ...     10.9003      0.5019   \n",
       "2      95.4878    0.1241    1.4436    0.0041  ...      9.2721      0.4958   \n",
       "3     104.2367    0.1217    1.4882   -0.0124  ...      8.5831      0.4990   \n",
       "4     100.3967    0.1235    1.5031   -0.0031  ...     10.9698      0.4800   \n",
       "...        ...       ...       ...       ...  ...         ...         ...   \n",
       "1562   82.2467    0.1248    1.3424   -0.0045  ...     11.7256      0.4988   \n",
       "1563   98.4689    0.1205    1.4333   -0.0061  ...     17.8379      0.4975   \n",
       "1564   99.4122    0.1208       NaN       NaN  ...     17.7267      0.4987   \n",
       "1565   98.7978    0.1213    1.4622   -0.0072  ...     19.2104      0.5004   \n",
       "1566   85.1011    0.1235       NaN       NaN  ...     22.9183      0.4987   \n",
       "\n",
       "      Sensor 431  Sensor 432  Sensor 433  Sensor 434  Sensor 435  Sensor 436  \\\n",
       "0         0.0118      0.0035      2.3630         NaN         NaN         NaN   \n",
       "1         0.0223      0.0055      4.4447      0.0096      0.0201      0.0060   \n",
       "2         0.0157      0.0039      3.1745      0.0584      0.0484      0.0148   \n",
       "3         0.0103      0.0025      2.0544      0.0202      0.0149      0.0044   \n",
       "4         0.4766      0.1045     99.3032      0.0202      0.0149      0.0044   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "1562      0.0143      0.0039      2.8669      0.0068      0.0138      0.0047   \n",
       "1563      0.0131      0.0036      2.6238      0.0068      0.0138      0.0047   \n",
       "1564      0.0153      0.0041      3.0590      0.0197      0.0086      0.0025   \n",
       "1565      0.0178      0.0038      3.5662      0.0262      0.0245      0.0075   \n",
       "1566      0.0181      0.0040      3.6275      0.0117      0.0162      0.0045   \n",
       "\n",
       "      Sensor 437  Pass/Fail  \n",
       "0            NaN       Pass  \n",
       "1       208.2045       Pass  \n",
       "2        82.8602       Fail  \n",
       "3        73.8432       Pass  \n",
       "4        73.8432       Pass  \n",
       "...          ...        ...  \n",
       "1562    203.1720       Pass  \n",
       "1563    203.1720       Pass  \n",
       "1564     43.5231       Pass  \n",
       "1565     93.4941       Pass  \n",
       "1566    137.7844       Pass  \n",
       "\n",
       "[1567 rows x 439 columns]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(dataset)\n",
    "#dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Quick Review of the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1567 entries, 0 to 1566\n",
      "Columns: 439 entries, Time to Pass/Fail\n",
      "dtypes: float64(437), object(2)\n",
      "memory usage: 5.2+ MB\n"
     ]
    }
   ],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape:\n",
      "(1567, 439)\n",
      "unique values:\n",
      "['Pass' 'Fail']\n",
      "printing size .....\n",
      "Pass    1463\n",
      "Fail     104\n",
      "Name: Pass/Fail, dtype: int64\n",
      "1463\n",
      "104\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x504 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x504 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Dataset shape:\")\n",
    "print (dataset.shape)\n",
    "unique_vals = dataset['Pass/Fail'].unique()\n",
    "print(\"unique values:\")\n",
    "print (unique_vals)\n",
    "\n",
    "#print (\"Only selecting rows with a Pass decision\")\n",
    "#print (dataset.loc[ dataset[\"Pass/Fail\"]==\"Pass\", : ])\n",
    "#print (dataset.loc[ dataset[\"Pass/Fail\"]==\"Pass\"]) # same as above line\n",
    "\n",
    "targets =  [dataset.loc[ dataset['Pass/Fail'] == val ]  for val in unique_vals]  \n",
    "#print (targets)\n",
    "#rint (targets[0]) # All passes\n",
    "#print (targets[1]) # All failes\n",
    "\n",
    "\n",
    "##### Pie chart##########\n",
    "labels = ['Pass', 'Fail']\n",
    "colors = ['Green', 'Red']\n",
    "size = dataset['Pass/Fail'].value_counts() # returns a python series (i.e. one dimensional labelled array) \n",
    "print (\"printing size .....\")\n",
    "print (size)\n",
    "print (size[\"Pass\"])\n",
    "print (size[\"Fail\"])\n",
    "explode = [0, 0.1 ]\n",
    "plt.style.use('seaborn-deep')\n",
    "plt.rcParams['figure.figsize'] = (7, 7)\n",
    "plt.pie(size, labels =labels, colors = colors, explode = explode, autopct = \"%.2f%%\" ,shadow = True)\n",
    "#plt.axis('off')\n",
    "plt.title('Target: Pass or Fail', fontsize = 15)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# series.plot(x='', y='', kind=‚Äúscatter‚Äù)\n",
    "size.plot(kind=\"bar\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seperate The Input and Output\n",
    "Here, we put the independent variables in X and the dependent variable in y. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.iloc[:, 1:438].values\n",
    "y = dataset.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Showing the Input Data in a Table format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 188
    },
    "colab_type": "code",
    "id": "hCsz2yCebe1R",
    "outputId": "1e4cc568-4e51-4b38-9d46-4aa3f15204be"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>427</th>\n",
       "      <th>428</th>\n",
       "      <th>429</th>\n",
       "      <th>430</th>\n",
       "      <th>431</th>\n",
       "      <th>432</th>\n",
       "      <th>433</th>\n",
       "      <th>434</th>\n",
       "      <th>435</th>\n",
       "      <th>436</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3030.93</td>\n",
       "      <td>2564.00</td>\n",
       "      <td>2187.7333</td>\n",
       "      <td>1411.1265</td>\n",
       "      <td>1.3602</td>\n",
       "      <td>97.6133</td>\n",
       "      <td>0.1242</td>\n",
       "      <td>1.5005</td>\n",
       "      <td>0.0162</td>\n",
       "      <td>-0.0034</td>\n",
       "      <td>...</td>\n",
       "      <td>1.6765</td>\n",
       "      <td>14.9509</td>\n",
       "      <td>0.5005</td>\n",
       "      <td>0.0118</td>\n",
       "      <td>0.0035</td>\n",
       "      <td>2.3630</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3095.78</td>\n",
       "      <td>2465.14</td>\n",
       "      <td>2230.4222</td>\n",
       "      <td>1463.6606</td>\n",
       "      <td>0.8294</td>\n",
       "      <td>102.3433</td>\n",
       "      <td>0.1247</td>\n",
       "      <td>1.4966</td>\n",
       "      <td>-0.0005</td>\n",
       "      <td>-0.0148</td>\n",
       "      <td>...</td>\n",
       "      <td>1.1065</td>\n",
       "      <td>10.9003</td>\n",
       "      <td>0.5019</td>\n",
       "      <td>0.0223</td>\n",
       "      <td>0.0055</td>\n",
       "      <td>4.4447</td>\n",
       "      <td>0.0096</td>\n",
       "      <td>0.0201</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>208.2045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2932.61</td>\n",
       "      <td>2559.94</td>\n",
       "      <td>2186.4111</td>\n",
       "      <td>1698.0172</td>\n",
       "      <td>1.5102</td>\n",
       "      <td>95.4878</td>\n",
       "      <td>0.1241</td>\n",
       "      <td>1.4436</td>\n",
       "      <td>0.0041</td>\n",
       "      <td>0.0013</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0952</td>\n",
       "      <td>9.2721</td>\n",
       "      <td>0.4958</td>\n",
       "      <td>0.0157</td>\n",
       "      <td>0.0039</td>\n",
       "      <td>3.1745</td>\n",
       "      <td>0.0584</td>\n",
       "      <td>0.0484</td>\n",
       "      <td>0.0148</td>\n",
       "      <td>82.8602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2988.72</td>\n",
       "      <td>2479.90</td>\n",
       "      <td>2199.0333</td>\n",
       "      <td>909.7926</td>\n",
       "      <td>1.3204</td>\n",
       "      <td>104.2367</td>\n",
       "      <td>0.1217</td>\n",
       "      <td>1.4882</td>\n",
       "      <td>-0.0124</td>\n",
       "      <td>-0.0033</td>\n",
       "      <td>...</td>\n",
       "      <td>1.7585</td>\n",
       "      <td>8.5831</td>\n",
       "      <td>0.4990</td>\n",
       "      <td>0.0103</td>\n",
       "      <td>0.0025</td>\n",
       "      <td>2.0544</td>\n",
       "      <td>0.0202</td>\n",
       "      <td>0.0149</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>73.8432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3032.24</td>\n",
       "      <td>2502.87</td>\n",
       "      <td>2233.3667</td>\n",
       "      <td>1326.5200</td>\n",
       "      <td>1.5334</td>\n",
       "      <td>100.3967</td>\n",
       "      <td>0.1235</td>\n",
       "      <td>1.5031</td>\n",
       "      <td>-0.0031</td>\n",
       "      <td>-0.0072</td>\n",
       "      <td>...</td>\n",
       "      <td>1.6597</td>\n",
       "      <td>10.9698</td>\n",
       "      <td>0.4800</td>\n",
       "      <td>0.4766</td>\n",
       "      <td>0.1045</td>\n",
       "      <td>99.3032</td>\n",
       "      <td>0.0202</td>\n",
       "      <td>0.0149</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>73.8432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1562</th>\n",
       "      <td>2899.41</td>\n",
       "      <td>2464.36</td>\n",
       "      <td>2179.7333</td>\n",
       "      <td>3085.3781</td>\n",
       "      <td>1.4843</td>\n",
       "      <td>82.2467</td>\n",
       "      <td>0.1248</td>\n",
       "      <td>1.3424</td>\n",
       "      <td>-0.0045</td>\n",
       "      <td>-0.0057</td>\n",
       "      <td>...</td>\n",
       "      <td>1.4879</td>\n",
       "      <td>11.7256</td>\n",
       "      <td>0.4988</td>\n",
       "      <td>0.0143</td>\n",
       "      <td>0.0039</td>\n",
       "      <td>2.8669</td>\n",
       "      <td>0.0068</td>\n",
       "      <td>0.0138</td>\n",
       "      <td>0.0047</td>\n",
       "      <td>203.1720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1563</th>\n",
       "      <td>3052.31</td>\n",
       "      <td>2522.55</td>\n",
       "      <td>2198.5667</td>\n",
       "      <td>1124.6595</td>\n",
       "      <td>0.8763</td>\n",
       "      <td>98.4689</td>\n",
       "      <td>0.1205</td>\n",
       "      <td>1.4333</td>\n",
       "      <td>-0.0061</td>\n",
       "      <td>-0.0093</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0187</td>\n",
       "      <td>17.8379</td>\n",
       "      <td>0.4975</td>\n",
       "      <td>0.0131</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>2.6238</td>\n",
       "      <td>0.0068</td>\n",
       "      <td>0.0138</td>\n",
       "      <td>0.0047</td>\n",
       "      <td>203.1720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1564</th>\n",
       "      <td>2978.81</td>\n",
       "      <td>2379.78</td>\n",
       "      <td>2206.3000</td>\n",
       "      <td>1110.4967</td>\n",
       "      <td>0.8236</td>\n",
       "      <td>99.4122</td>\n",
       "      <td>0.1208</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.2237</td>\n",
       "      <td>17.7267</td>\n",
       "      <td>0.4987</td>\n",
       "      <td>0.0153</td>\n",
       "      <td>0.0041</td>\n",
       "      <td>3.0590</td>\n",
       "      <td>0.0197</td>\n",
       "      <td>0.0086</td>\n",
       "      <td>0.0025</td>\n",
       "      <td>43.5231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1565</th>\n",
       "      <td>2894.92</td>\n",
       "      <td>2532.01</td>\n",
       "      <td>2177.0333</td>\n",
       "      <td>1183.7287</td>\n",
       "      <td>1.5726</td>\n",
       "      <td>98.7978</td>\n",
       "      <td>0.1213</td>\n",
       "      <td>1.4622</td>\n",
       "      <td>-0.0072</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>...</td>\n",
       "      <td>1.7085</td>\n",
       "      <td>19.2104</td>\n",
       "      <td>0.5004</td>\n",
       "      <td>0.0178</td>\n",
       "      <td>0.0038</td>\n",
       "      <td>3.5662</td>\n",
       "      <td>0.0262</td>\n",
       "      <td>0.0245</td>\n",
       "      <td>0.0075</td>\n",
       "      <td>93.4941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1566</th>\n",
       "      <td>2944.92</td>\n",
       "      <td>2450.76</td>\n",
       "      <td>2195.4444</td>\n",
       "      <td>2914.1792</td>\n",
       "      <td>1.5978</td>\n",
       "      <td>85.1011</td>\n",
       "      <td>0.1235</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.2878</td>\n",
       "      <td>22.9183</td>\n",
       "      <td>0.4987</td>\n",
       "      <td>0.0181</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>3.6275</td>\n",
       "      <td>0.0117</td>\n",
       "      <td>0.0162</td>\n",
       "      <td>0.0045</td>\n",
       "      <td>137.7844</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1567 rows √ó 437 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0        1          2          3       4         5       6    \\\n",
       "0     3030.93  2564.00  2187.7333  1411.1265  1.3602   97.6133  0.1242   \n",
       "1     3095.78  2465.14  2230.4222  1463.6606  0.8294  102.3433  0.1247   \n",
       "2     2932.61  2559.94  2186.4111  1698.0172  1.5102   95.4878  0.1241   \n",
       "3     2988.72  2479.90  2199.0333   909.7926  1.3204  104.2367  0.1217   \n",
       "4     3032.24  2502.87  2233.3667  1326.5200  1.5334  100.3967  0.1235   \n",
       "...       ...      ...        ...        ...     ...       ...     ...   \n",
       "1562  2899.41  2464.36  2179.7333  3085.3781  1.4843   82.2467  0.1248   \n",
       "1563  3052.31  2522.55  2198.5667  1124.6595  0.8763   98.4689  0.1205   \n",
       "1564  2978.81  2379.78  2206.3000  1110.4967  0.8236   99.4122  0.1208   \n",
       "1565  2894.92  2532.01  2177.0333  1183.7287  1.5726   98.7978  0.1213   \n",
       "1566  2944.92  2450.76  2195.4444  2914.1792  1.5978   85.1011  0.1235   \n",
       "\n",
       "         7       8       9    ...     427      428     429     430     431  \\\n",
       "0     1.5005  0.0162 -0.0034  ...  1.6765  14.9509  0.5005  0.0118  0.0035   \n",
       "1     1.4966 -0.0005 -0.0148  ...  1.1065  10.9003  0.5019  0.0223  0.0055   \n",
       "2     1.4436  0.0041  0.0013  ...  2.0952   9.2721  0.4958  0.0157  0.0039   \n",
       "3     1.4882 -0.0124 -0.0033  ...  1.7585   8.5831  0.4990  0.0103  0.0025   \n",
       "4     1.5031 -0.0031 -0.0072  ...  1.6597  10.9698  0.4800  0.4766  0.1045   \n",
       "...      ...     ...     ...  ...     ...      ...     ...     ...     ...   \n",
       "1562  1.3424 -0.0045 -0.0057  ...  1.4879  11.7256  0.4988  0.0143  0.0039   \n",
       "1563  1.4333 -0.0061 -0.0093  ...  1.0187  17.8379  0.4975  0.0131  0.0036   \n",
       "1564     NaN     NaN     NaN  ...  1.2237  17.7267  0.4987  0.0153  0.0041   \n",
       "1565  1.4622 -0.0072  0.0032  ...  1.7085  19.2104  0.5004  0.0178  0.0038   \n",
       "1566     NaN     NaN     NaN  ...  1.2878  22.9183  0.4987  0.0181  0.0040   \n",
       "\n",
       "          432     433     434     435       436  \n",
       "0      2.3630     NaN     NaN     NaN       NaN  \n",
       "1      4.4447  0.0096  0.0201  0.0060  208.2045  \n",
       "2      3.1745  0.0584  0.0484  0.0148   82.8602  \n",
       "3      2.0544  0.0202  0.0149  0.0044   73.8432  \n",
       "4     99.3032  0.0202  0.0149  0.0044   73.8432  \n",
       "...       ...     ...     ...     ...       ...  \n",
       "1562   2.8669  0.0068  0.0138  0.0047  203.1720  \n",
       "1563   2.6238  0.0068  0.0138  0.0047  203.1720  \n",
       "1564   3.0590  0.0197  0.0086  0.0025   43.5231  \n",
       "1565   3.5662  0.0262  0.0245  0.0075   93.4941  \n",
       "1566   3.6275  0.0117  0.0162  0.0045  137.7844  \n",
       "\n",
       "[1567 rows x 437 columns]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Quick Check of the Output Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "eYrOQ43XcJR3",
    "outputId": "e0873b2a-3b08-4bab-ef0d-15b88858ca44"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1562</th>\n",
       "      <td>Pass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1563</th>\n",
       "      <td>Pass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1564</th>\n",
       "      <td>Pass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1565</th>\n",
       "      <td>Pass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1566</th>\n",
       "      <td>Pass</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1567 rows √ó 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0\n",
       "0     Pass\n",
       "1     Pass\n",
       "2     Fail\n",
       "3     Pass\n",
       "4     Pass\n",
       "...    ...\n",
       "1562  Pass\n",
       "1563  Pass\n",
       "1564  Pass\n",
       "1565  Pass\n",
       "1566  Pass\n",
       "\n",
       "[1567 rows x 1 columns]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nhfKXNxlSabC"
   },
   "source": [
    "## Taking care of missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c93k7ipkSexq"
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imputer.fit(X)\n",
    "X = imputer.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 188
    },
    "colab_type": "code",
    "id": "3UgLdMS_bjq_",
    "outputId": "254af4e0-681e-47f5-aaa7-b9c6f43258e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.03093000e+03 2.56400000e+03 2.18773330e+03 ... 1.64749042e-02\n",
      "  5.28333333e-03 9.96700663e+01]\n",
      " [3.09578000e+03 2.46514000e+03 2.23042220e+03 ... 2.01000000e-02\n",
      "  6.00000000e-03 2.08204500e+02]\n",
      " [2.93261000e+03 2.55994000e+03 2.18641110e+03 ... 4.84000000e-02\n",
      "  1.48000000e-02 8.28602000e+01]\n",
      " ...\n",
      " [2.97881000e+03 2.37978000e+03 2.20630000e+03 ... 8.60000000e-03\n",
      "  2.50000000e-03 4.35231000e+01]\n",
      " [2.89492000e+03 2.53201000e+03 2.17703330e+03 ... 2.45000000e-02\n",
      "  7.50000000e-03 9.34941000e+01]\n",
      " [2.94492000e+03 2.45076000e+03 2.19544440e+03 ... 1.62000000e-02\n",
      "  4.50000000e-03 1.37784400e+02]]\n"
     ]
    }
   ],
   "source": [
    "# A quick check\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking if X contains any NULL values\n",
    "pd.DataFrame(X).isnull().any().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CriG6VzVSjcK"
   },
   "source": [
    "## Encoding Categorical Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AhSpdQWeSsFh"
   },
   "source": [
    "### Encoding the Independent Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5hwuVddlSwVi"
   },
   "outputs": [],
   "source": [
    "# we don't have any categorical data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DXh8oVSITIc6"
   },
   "source": [
    "### Encoding the Dependent Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XgHCShVyTOYY"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "FyhY8-gPpFCa",
    "outputId": "7f76ef29-5423-4c3e-cf69-45fbc366a997"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 0 ... 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# a qucik check\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary of the output variable encoding :\n",
    "\n",
    "Pass: 1\n",
    "\n",
    "Fail :0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taking care of outliers in X & y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask shape:  (1567,)\n",
      "X shape (1551, 437)\n",
      "y shape (1551,)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>427</th>\n",
       "      <th>428</th>\n",
       "      <th>429</th>\n",
       "      <th>430</th>\n",
       "      <th>431</th>\n",
       "      <th>432</th>\n",
       "      <th>433</th>\n",
       "      <th>434</th>\n",
       "      <th>435</th>\n",
       "      <th>436</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3030.93</td>\n",
       "      <td>2564.00</td>\n",
       "      <td>2187.7333</td>\n",
       "      <td>1411.1265</td>\n",
       "      <td>1.3602</td>\n",
       "      <td>97.6133</td>\n",
       "      <td>0.1242</td>\n",
       "      <td>1.500500</td>\n",
       "      <td>0.016200</td>\n",
       "      <td>-0.003400</td>\n",
       "      <td>...</td>\n",
       "      <td>1.6765</td>\n",
       "      <td>14.9509</td>\n",
       "      <td>0.5005</td>\n",
       "      <td>0.0118</td>\n",
       "      <td>0.0035</td>\n",
       "      <td>2.3630</td>\n",
       "      <td>0.021458</td>\n",
       "      <td>0.016475</td>\n",
       "      <td>0.005283</td>\n",
       "      <td>99.670066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3095.78</td>\n",
       "      <td>2465.14</td>\n",
       "      <td>2230.4222</td>\n",
       "      <td>1463.6606</td>\n",
       "      <td>0.8294</td>\n",
       "      <td>102.3433</td>\n",
       "      <td>0.1247</td>\n",
       "      <td>1.496600</td>\n",
       "      <td>-0.000500</td>\n",
       "      <td>-0.014800</td>\n",
       "      <td>...</td>\n",
       "      <td>1.1065</td>\n",
       "      <td>10.9003</td>\n",
       "      <td>0.5019</td>\n",
       "      <td>0.0223</td>\n",
       "      <td>0.0055</td>\n",
       "      <td>4.4447</td>\n",
       "      <td>0.009600</td>\n",
       "      <td>0.020100</td>\n",
       "      <td>0.006000</td>\n",
       "      <td>208.204500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2932.61</td>\n",
       "      <td>2559.94</td>\n",
       "      <td>2186.4111</td>\n",
       "      <td>1698.0172</td>\n",
       "      <td>1.5102</td>\n",
       "      <td>95.4878</td>\n",
       "      <td>0.1241</td>\n",
       "      <td>1.443600</td>\n",
       "      <td>0.004100</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0952</td>\n",
       "      <td>9.2721</td>\n",
       "      <td>0.4958</td>\n",
       "      <td>0.0157</td>\n",
       "      <td>0.0039</td>\n",
       "      <td>3.1745</td>\n",
       "      <td>0.058400</td>\n",
       "      <td>0.048400</td>\n",
       "      <td>0.014800</td>\n",
       "      <td>82.860200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2988.72</td>\n",
       "      <td>2479.90</td>\n",
       "      <td>2199.0333</td>\n",
       "      <td>909.7926</td>\n",
       "      <td>1.3204</td>\n",
       "      <td>104.2367</td>\n",
       "      <td>0.1217</td>\n",
       "      <td>1.488200</td>\n",
       "      <td>-0.012400</td>\n",
       "      <td>-0.003300</td>\n",
       "      <td>...</td>\n",
       "      <td>1.7585</td>\n",
       "      <td>8.5831</td>\n",
       "      <td>0.4990</td>\n",
       "      <td>0.0103</td>\n",
       "      <td>0.0025</td>\n",
       "      <td>2.0544</td>\n",
       "      <td>0.020200</td>\n",
       "      <td>0.014900</td>\n",
       "      <td>0.004400</td>\n",
       "      <td>73.843200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3032.24</td>\n",
       "      <td>2502.87</td>\n",
       "      <td>2233.3667</td>\n",
       "      <td>1326.5200</td>\n",
       "      <td>1.5334</td>\n",
       "      <td>100.3967</td>\n",
       "      <td>0.1235</td>\n",
       "      <td>1.503100</td>\n",
       "      <td>-0.003100</td>\n",
       "      <td>-0.007200</td>\n",
       "      <td>...</td>\n",
       "      <td>1.6597</td>\n",
       "      <td>10.9698</td>\n",
       "      <td>0.4800</td>\n",
       "      <td>0.4766</td>\n",
       "      <td>0.1045</td>\n",
       "      <td>99.3032</td>\n",
       "      <td>0.020200</td>\n",
       "      <td>0.014900</td>\n",
       "      <td>0.004400</td>\n",
       "      <td>73.843200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1546</th>\n",
       "      <td>2899.41</td>\n",
       "      <td>2464.36</td>\n",
       "      <td>2179.7333</td>\n",
       "      <td>3085.3781</td>\n",
       "      <td>1.4843</td>\n",
       "      <td>82.2467</td>\n",
       "      <td>0.1248</td>\n",
       "      <td>1.342400</td>\n",
       "      <td>-0.004500</td>\n",
       "      <td>-0.005700</td>\n",
       "      <td>...</td>\n",
       "      <td>1.4879</td>\n",
       "      <td>11.7256</td>\n",
       "      <td>0.4988</td>\n",
       "      <td>0.0143</td>\n",
       "      <td>0.0039</td>\n",
       "      <td>2.8669</td>\n",
       "      <td>0.006800</td>\n",
       "      <td>0.013800</td>\n",
       "      <td>0.004700</td>\n",
       "      <td>203.172000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1547</th>\n",
       "      <td>3052.31</td>\n",
       "      <td>2522.55</td>\n",
       "      <td>2198.5667</td>\n",
       "      <td>1124.6595</td>\n",
       "      <td>0.8763</td>\n",
       "      <td>98.4689</td>\n",
       "      <td>0.1205</td>\n",
       "      <td>1.433300</td>\n",
       "      <td>-0.006100</td>\n",
       "      <td>-0.009300</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0187</td>\n",
       "      <td>17.8379</td>\n",
       "      <td>0.4975</td>\n",
       "      <td>0.0131</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>2.6238</td>\n",
       "      <td>0.006800</td>\n",
       "      <td>0.013800</td>\n",
       "      <td>0.004700</td>\n",
       "      <td>203.172000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1548</th>\n",
       "      <td>2978.81</td>\n",
       "      <td>2379.78</td>\n",
       "      <td>2206.3000</td>\n",
       "      <td>1110.4967</td>\n",
       "      <td>0.8236</td>\n",
       "      <td>99.4122</td>\n",
       "      <td>0.1208</td>\n",
       "      <td>1.462862</td>\n",
       "      <td>-0.000841</td>\n",
       "      <td>0.000146</td>\n",
       "      <td>...</td>\n",
       "      <td>1.2237</td>\n",
       "      <td>17.7267</td>\n",
       "      <td>0.4987</td>\n",
       "      <td>0.0153</td>\n",
       "      <td>0.0041</td>\n",
       "      <td>3.0590</td>\n",
       "      <td>0.019700</td>\n",
       "      <td>0.008600</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>43.523100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1549</th>\n",
       "      <td>2894.92</td>\n",
       "      <td>2532.01</td>\n",
       "      <td>2177.0333</td>\n",
       "      <td>1183.7287</td>\n",
       "      <td>1.5726</td>\n",
       "      <td>98.7978</td>\n",
       "      <td>0.1213</td>\n",
       "      <td>1.462200</td>\n",
       "      <td>-0.007200</td>\n",
       "      <td>0.003200</td>\n",
       "      <td>...</td>\n",
       "      <td>1.7085</td>\n",
       "      <td>19.2104</td>\n",
       "      <td>0.5004</td>\n",
       "      <td>0.0178</td>\n",
       "      <td>0.0038</td>\n",
       "      <td>3.5662</td>\n",
       "      <td>0.026200</td>\n",
       "      <td>0.024500</td>\n",
       "      <td>0.007500</td>\n",
       "      <td>93.494100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1550</th>\n",
       "      <td>2944.92</td>\n",
       "      <td>2450.76</td>\n",
       "      <td>2195.4444</td>\n",
       "      <td>2914.1792</td>\n",
       "      <td>1.5978</td>\n",
       "      <td>85.1011</td>\n",
       "      <td>0.1235</td>\n",
       "      <td>1.462862</td>\n",
       "      <td>-0.000841</td>\n",
       "      <td>0.000146</td>\n",
       "      <td>...</td>\n",
       "      <td>1.2878</td>\n",
       "      <td>22.9183</td>\n",
       "      <td>0.4987</td>\n",
       "      <td>0.0181</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>3.6275</td>\n",
       "      <td>0.011700</td>\n",
       "      <td>0.016200</td>\n",
       "      <td>0.004500</td>\n",
       "      <td>137.784400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1551 rows √ó 437 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0        1          2          3       4         5       6    \\\n",
       "0     3030.93  2564.00  2187.7333  1411.1265  1.3602   97.6133  0.1242   \n",
       "1     3095.78  2465.14  2230.4222  1463.6606  0.8294  102.3433  0.1247   \n",
       "2     2932.61  2559.94  2186.4111  1698.0172  1.5102   95.4878  0.1241   \n",
       "3     2988.72  2479.90  2199.0333   909.7926  1.3204  104.2367  0.1217   \n",
       "4     3032.24  2502.87  2233.3667  1326.5200  1.5334  100.3967  0.1235   \n",
       "...       ...      ...        ...        ...     ...       ...     ...   \n",
       "1546  2899.41  2464.36  2179.7333  3085.3781  1.4843   82.2467  0.1248   \n",
       "1547  3052.31  2522.55  2198.5667  1124.6595  0.8763   98.4689  0.1205   \n",
       "1548  2978.81  2379.78  2206.3000  1110.4967  0.8236   99.4122  0.1208   \n",
       "1549  2894.92  2532.01  2177.0333  1183.7287  1.5726   98.7978  0.1213   \n",
       "1550  2944.92  2450.76  2195.4444  2914.1792  1.5978   85.1011  0.1235   \n",
       "\n",
       "           7         8         9    ...     427      428     429     430  \\\n",
       "0     1.500500  0.016200 -0.003400  ...  1.6765  14.9509  0.5005  0.0118   \n",
       "1     1.496600 -0.000500 -0.014800  ...  1.1065  10.9003  0.5019  0.0223   \n",
       "2     1.443600  0.004100  0.001300  ...  2.0952   9.2721  0.4958  0.0157   \n",
       "3     1.488200 -0.012400 -0.003300  ...  1.7585   8.5831  0.4990  0.0103   \n",
       "4     1.503100 -0.003100 -0.007200  ...  1.6597  10.9698  0.4800  0.4766   \n",
       "...        ...       ...       ...  ...     ...      ...     ...     ...   \n",
       "1546  1.342400 -0.004500 -0.005700  ...  1.4879  11.7256  0.4988  0.0143   \n",
       "1547  1.433300 -0.006100 -0.009300  ...  1.0187  17.8379  0.4975  0.0131   \n",
       "1548  1.462862 -0.000841  0.000146  ...  1.2237  17.7267  0.4987  0.0153   \n",
       "1549  1.462200 -0.007200  0.003200  ...  1.7085  19.2104  0.5004  0.0178   \n",
       "1550  1.462862 -0.000841  0.000146  ...  1.2878  22.9183  0.4987  0.0181   \n",
       "\n",
       "         431      432       433       434       435         436  \n",
       "0     0.0035   2.3630  0.021458  0.016475  0.005283   99.670066  \n",
       "1     0.0055   4.4447  0.009600  0.020100  0.006000  208.204500  \n",
       "2     0.0039   3.1745  0.058400  0.048400  0.014800   82.860200  \n",
       "3     0.0025   2.0544  0.020200  0.014900  0.004400   73.843200  \n",
       "4     0.1045  99.3032  0.020200  0.014900  0.004400   73.843200  \n",
       "...      ...      ...       ...       ...       ...         ...  \n",
       "1546  0.0039   2.8669  0.006800  0.013800  0.004700  203.172000  \n",
       "1547  0.0036   2.6238  0.006800  0.013800  0.004700  203.172000  \n",
       "1548  0.0041   3.0590  0.019700  0.008600  0.002500   43.523100  \n",
       "1549  0.0038   3.5662  0.026200  0.024500  0.007500   93.494100  \n",
       "1550  0.0040   3.6275  0.011700  0.016200  0.004500  137.784400  \n",
       "\n",
       "[1551 rows x 437 columns]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Source 1: https://towardsdatascience.com/local-outlier-factor-lof-algorithm-for-outlier-identification-8efb887d9843\n",
    "# Source 2: https://towardsdatascience.com/anomaly-detection-with-local-outlier-factor-lof-d91e41df10f2\n",
    "#Local Outlier Factor (LOF) ‚Äî Algorithm for outlier identification\n",
    "\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "lof = LocalOutlierFactor(contamination=0.01)\n",
    "\n",
    "# yhat has postive and negative values(i.e +1 & -1).Negative values are outliers \n",
    "# and positives inliers\n",
    "\n",
    "yhat = lof.fit_predict(X)\n",
    "\n",
    "\n",
    "# filter outlier values\n",
    "mask = (yhat != -1)\n",
    "\n",
    "print(\"mask shape: \",mask.shape)\n",
    "\n",
    "X, y = X[mask, :], y[mask]\n",
    "print(\"X shape\",X.shape)\n",
    "print(\"y shape\",y.shape)\n",
    "\n",
    "pd.DataFrame(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qb_vcgm3qZKW"
   },
   "source": [
    "## Splitting the Dataset into the Training set and Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pXgA6CzlqbCl"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TpGqbS4TqkIR"
   },
   "source": [
    "## Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AxjSUXFQqo-3"
   },
   "outputs": [],
   "source": [
    "# Scaling the X_train using fit_transform () and scaling X_train using the transform () \n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>427</th>\n",
       "      <th>428</th>\n",
       "      <th>429</th>\n",
       "      <th>430</th>\n",
       "      <th>431</th>\n",
       "      <th>432</th>\n",
       "      <th>433</th>\n",
       "      <th>434</th>\n",
       "      <th>435</th>\n",
       "      <th>436</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-2.072180</td>\n",
       "      <td>0.475795</td>\n",
       "      <td>0.805546</td>\n",
       "      <td>0.269758</td>\n",
       "      <td>0.397772</td>\n",
       "      <td>-0.222463</td>\n",
       "      <td>-0.258889</td>\n",
       "      <td>-1.187515</td>\n",
       "      <td>0.035521</td>\n",
       "      <td>0.259533</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.206552</td>\n",
       "      <td>-0.462763</td>\n",
       "      <td>0.741028</td>\n",
       "      <td>0.341120</td>\n",
       "      <td>0.209042</td>\n",
       "      <td>0.317386</td>\n",
       "      <td>0.628919</td>\n",
       "      <td>0.809275</td>\n",
       "      <td>0.627246</td>\n",
       "      <td>-0.201607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-3.047690</td>\n",
       "      <td>0.475919</td>\n",
       "      <td>-0.647662</td>\n",
       "      <td>-0.538189</td>\n",
       "      <td>-0.686511</td>\n",
       "      <td>-0.367036</td>\n",
       "      <td>0.028874</td>\n",
       "      <td>1.110874</td>\n",
       "      <td>-0.942923</td>\n",
       "      <td>-0.264703</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.208999</td>\n",
       "      <td>-0.224070</td>\n",
       "      <td>1.072751</td>\n",
       "      <td>0.233177</td>\n",
       "      <td>0.242327</td>\n",
       "      <td>0.210641</td>\n",
       "      <td>0.955127</td>\n",
       "      <td>0.505658</td>\n",
       "      <td>0.245556</td>\n",
       "      <td>-0.389124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.747138</td>\n",
       "      <td>0.229741</td>\n",
       "      <td>0.984539</td>\n",
       "      <td>0.369367</td>\n",
       "      <td>0.300398</td>\n",
       "      <td>0.254969</td>\n",
       "      <td>0.191523</td>\n",
       "      <td>0.065422</td>\n",
       "      <td>-0.079989</td>\n",
       "      <td>1.067731</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.246899</td>\n",
       "      <td>-0.489487</td>\n",
       "      <td>-1.581035</td>\n",
       "      <td>-0.328122</td>\n",
       "      <td>-0.157099</td>\n",
       "      <td>-0.309888</td>\n",
       "      <td>-0.602515</td>\n",
       "      <td>-0.596358</td>\n",
       "      <td>-0.517823</td>\n",
       "      <td>-0.210724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.479892</td>\n",
       "      <td>0.261176</td>\n",
       "      <td>0.298769</td>\n",
       "      <td>-1.055695</td>\n",
       "      <td>1.167585</td>\n",
       "      <td>2.937396</td>\n",
       "      <td>0.028874</td>\n",
       "      <td>2.092074</td>\n",
       "      <td>-0.793438</td>\n",
       "      <td>-1.793725</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.223270</td>\n",
       "      <td>0.094469</td>\n",
       "      <td>-1.038215</td>\n",
       "      <td>-0.184199</td>\n",
       "      <td>-0.190384</td>\n",
       "      <td>-0.171500</td>\n",
       "      <td>0.131453</td>\n",
       "      <td>0.505658</td>\n",
       "      <td>0.002663</td>\n",
       "      <td>-0.097077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.437808</td>\n",
       "      <td>0.571949</td>\n",
       "      <td>1.112821</td>\n",
       "      <td>0.096782</td>\n",
       "      <td>0.313368</td>\n",
       "      <td>0.235726</td>\n",
       "      <td>0.191523</td>\n",
       "      <td>0.298339</td>\n",
       "      <td>-1.194328</td>\n",
       "      <td>-0.756174</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.223480</td>\n",
       "      <td>-0.513534</td>\n",
       "      <td>0.379148</td>\n",
       "      <td>-0.392888</td>\n",
       "      <td>-0.390097</td>\n",
       "      <td>-0.382572</td>\n",
       "      <td>-0.488342</td>\n",
       "      <td>-0.585113</td>\n",
       "      <td>-0.448425</td>\n",
       "      <td>-0.281496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1235</th>\n",
       "      <td>-0.156888</td>\n",
       "      <td>-0.782706</td>\n",
       "      <td>-2.134059</td>\n",
       "      <td>-0.507795</td>\n",
       "      <td>-1.159014</td>\n",
       "      <td>0.883449</td>\n",
       "      <td>0.316638</td>\n",
       "      <td>0.637007</td>\n",
       "      <td>0.558717</td>\n",
       "      <td>-0.242860</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.189887</td>\n",
       "      <td>-0.452690</td>\n",
       "      <td>-0.917589</td>\n",
       "      <td>-0.227376</td>\n",
       "      <td>-0.123813</td>\n",
       "      <td>-0.214923</td>\n",
       "      <td>-0.015341</td>\n",
       "      <td>-0.630093</td>\n",
       "      <td>-0.587221</td>\n",
       "      <td>-0.510591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1236</th>\n",
       "      <td>0.010882</td>\n",
       "      <td>0.610040</td>\n",
       "      <td>1.563288</td>\n",
       "      <td>1.315059</td>\n",
       "      <td>-0.389600</td>\n",
       "      <td>-0.727587</td>\n",
       "      <td>-0.133775</td>\n",
       "      <td>0.440232</td>\n",
       "      <td>1.000376</td>\n",
       "      <td>0.543495</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.218531</td>\n",
       "      <td>0.538174</td>\n",
       "      <td>0.137895</td>\n",
       "      <td>-0.033080</td>\n",
       "      <td>-0.023957</td>\n",
       "      <td>-0.036047</td>\n",
       "      <td>0.522902</td>\n",
       "      <td>-0.337721</td>\n",
       "      <td>-0.379026</td>\n",
       "      <td>-0.542377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1237</th>\n",
       "      <td>0.555081</td>\n",
       "      <td>-0.465153</td>\n",
       "      <td>-0.889676</td>\n",
       "      <td>-0.569491</td>\n",
       "      <td>-0.841152</td>\n",
       "      <td>0.461414</td>\n",
       "      <td>0.341660</td>\n",
       "      <td>0.263536</td>\n",
       "      <td>0.388848</td>\n",
       "      <td>1.581046</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.261026</td>\n",
       "      <td>-0.269581</td>\n",
       "      <td>-2.244482</td>\n",
       "      <td>-0.105042</td>\n",
       "      <td>0.075900</td>\n",
       "      <td>-0.091009</td>\n",
       "      <td>-0.912412</td>\n",
       "      <td>-0.360212</td>\n",
       "      <td>0.002663</td>\n",
       "      <td>0.303563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1238</th>\n",
       "      <td>-0.497725</td>\n",
       "      <td>0.287063</td>\n",
       "      <td>-1.198069</td>\n",
       "      <td>-0.833035</td>\n",
       "      <td>0.001691</td>\n",
       "      <td>0.150128</td>\n",
       "      <td>0.279103</td>\n",
       "      <td>0.359915</td>\n",
       "      <td>0.103469</td>\n",
       "      <td>0.598103</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.174225</td>\n",
       "      <td>1.983292</td>\n",
       "      <td>1.253691</td>\n",
       "      <td>-0.061865</td>\n",
       "      <td>-0.057242</td>\n",
       "      <td>-0.067138</td>\n",
       "      <td>0.433195</td>\n",
       "      <td>-0.348967</td>\n",
       "      <td>-0.170832</td>\n",
       "      <td>-0.526085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1239</th>\n",
       "      <td>0.125265</td>\n",
       "      <td>-2.228830</td>\n",
       "      <td>-0.698004</td>\n",
       "      <td>-1.191626</td>\n",
       "      <td>-0.181483</td>\n",
       "      <td>0.931467</td>\n",
       "      <td>-0.083729</td>\n",
       "      <td>-0.518211</td>\n",
       "      <td>0.585896</td>\n",
       "      <td>-0.636037</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.218963</td>\n",
       "      <td>0.063178</td>\n",
       "      <td>-0.495395</td>\n",
       "      <td>-0.241768</td>\n",
       "      <td>-0.090528</td>\n",
       "      <td>-0.229259</td>\n",
       "      <td>-0.488342</td>\n",
       "      <td>-0.585113</td>\n",
       "      <td>-0.448425</td>\n",
       "      <td>-0.281496</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1240 rows √ó 437 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6    \\\n",
       "0    -2.072180  0.475795  0.805546  0.269758  0.397772 -0.222463 -0.258889   \n",
       "1    -3.047690  0.475919 -0.647662 -0.538189 -0.686511 -0.367036  0.028874   \n",
       "2    -0.747138  0.229741  0.984539  0.369367  0.300398  0.254969  0.191523   \n",
       "3    -1.479892  0.261176  0.298769 -1.055695  1.167585  2.937396  0.028874   \n",
       "4     1.437808  0.571949  1.112821  0.096782  0.313368  0.235726  0.191523   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "1235 -0.156888 -0.782706 -2.134059 -0.507795 -1.159014  0.883449  0.316638   \n",
       "1236  0.010882  0.610040  1.563288  1.315059 -0.389600 -0.727587 -0.133775   \n",
       "1237  0.555081 -0.465153 -0.889676 -0.569491 -0.841152  0.461414  0.341660   \n",
       "1238 -0.497725  0.287063 -1.198069 -0.833035  0.001691  0.150128  0.279103   \n",
       "1239  0.125265 -2.228830 -0.698004 -1.191626 -0.181483  0.931467 -0.083729   \n",
       "\n",
       "           7         8         9    ...       427       428       429  \\\n",
       "0    -1.187515  0.035521  0.259533  ... -0.206552 -0.462763  0.741028   \n",
       "1     1.110874 -0.942923 -0.264703  ... -0.208999 -0.224070  1.072751   \n",
       "2     0.065422 -0.079989  1.067731  ... -0.246899 -0.489487 -1.581035   \n",
       "3     2.092074 -0.793438 -1.793725  ... -0.223270  0.094469 -1.038215   \n",
       "4     0.298339 -1.194328 -0.756174  ... -0.223480 -0.513534  0.379148   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "1235  0.637007  0.558717 -0.242860  ... -0.189887 -0.452690 -0.917589   \n",
       "1236  0.440232  1.000376  0.543495  ... -0.218531  0.538174  0.137895   \n",
       "1237  0.263536  0.388848  1.581046  ... -0.261026 -0.269581 -2.244482   \n",
       "1238  0.359915  0.103469  0.598103  ... -0.174225  1.983292  1.253691   \n",
       "1239 -0.518211  0.585896 -0.636037  ... -0.218963  0.063178 -0.495395   \n",
       "\n",
       "           430       431       432       433       434       435       436  \n",
       "0     0.341120  0.209042  0.317386  0.628919  0.809275  0.627246 -0.201607  \n",
       "1     0.233177  0.242327  0.210641  0.955127  0.505658  0.245556 -0.389124  \n",
       "2    -0.328122 -0.157099 -0.309888 -0.602515 -0.596358 -0.517823 -0.210724  \n",
       "3    -0.184199 -0.190384 -0.171500  0.131453  0.505658  0.002663 -0.097077  \n",
       "4    -0.392888 -0.390097 -0.382572 -0.488342 -0.585113 -0.448425 -0.281496  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "1235 -0.227376 -0.123813 -0.214923 -0.015341 -0.630093 -0.587221 -0.510591  \n",
       "1236 -0.033080 -0.023957 -0.036047  0.522902 -0.337721 -0.379026 -0.542377  \n",
       "1237 -0.105042  0.075900 -0.091009 -0.912412 -0.360212  0.002663  0.303563  \n",
       "1238 -0.061865 -0.057242 -0.067138  0.433195 -0.348967 -0.170832 -0.526085  \n",
       "1239 -0.241768 -0.090528 -0.229259 -0.488342 -0.585113 -0.448425 -0.281496  \n",
       "\n",
       "[1240 rows x 437 columns]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Logistic Regression Model  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the Logistic Regression model on the train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=1000)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "LogitReg = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "LogitReg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating the Probability Estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape:  (311, 2)\n",
      " Probability estimates:  [[2.27948187e-01 7.72051813e-01]\n",
      " [5.36218054e-02 9.46378195e-01]\n",
      " [1.49094034e-02 9.85090597e-01]\n",
      " [0.00000000e+00 1.00000000e+00]\n",
      " [9.31272065e-07 9.99999069e-01]\n",
      " [2.68452287e-03 9.97315477e-01]\n",
      " [1.31412914e-05 9.99986859e-01]\n",
      " [1.97477935e-04 9.99802522e-01]\n",
      " [2.01263268e-07 9.99999799e-01]\n",
      " [1.19340428e-01 8.80659572e-01]\n",
      " [4.65147953e-01 5.34852047e-01]\n",
      " [4.46745287e-08 9.99999955e-01]\n",
      " [1.08564635e-05 9.99989144e-01]\n",
      " [4.93707693e-02 9.50629231e-01]\n",
      " [1.65287342e-06 9.99998347e-01]\n",
      " [1.23196211e-06 9.99998768e-01]\n",
      " [2.02875124e-04 9.99797125e-01]\n",
      " [8.38294324e-07 9.99999162e-01]\n",
      " [2.00474232e-02 9.79952577e-01]\n",
      " [4.75219448e-03 9.95247806e-01]\n",
      " [7.43249692e-02 9.25675031e-01]\n",
      " [1.06208569e-02 9.89379143e-01]\n",
      " [1.22417253e-07 9.99999878e-01]\n",
      " [1.39140595e-04 9.99860859e-01]\n",
      " [1.16477467e-06 9.99998835e-01]\n",
      " [7.36224795e-04 9.99263775e-01]\n",
      " [5.37477619e-05 9.99946252e-01]\n",
      " [9.93564993e-01 6.43500659e-03]\n",
      " [6.14416037e-02 9.38558396e-01]\n",
      " [1.80200373e-02 9.81979963e-01]\n",
      " [4.79194602e-01 5.20805398e-01]\n",
      " [6.06485672e-05 9.99939351e-01]\n",
      " [1.27067171e-02 9.87293283e-01]\n",
      " [9.22809853e-06 9.99990772e-01]\n",
      " [4.64406732e-05 9.99953559e-01]\n",
      " [7.61267977e-07 9.99999239e-01]\n",
      " [1.43663598e-02 9.85633640e-01]\n",
      " [1.00105167e-05 9.99989989e-01]\n",
      " [6.16732390e-03 9.93832676e-01]\n",
      " [9.48937097e-05 9.99905106e-01]\n",
      " [1.02016449e-05 9.99989798e-01]\n",
      " [6.27726034e-06 9.99993723e-01]\n",
      " [3.23653833e-06 9.99996763e-01]\n",
      " [2.93368529e-03 9.97066315e-01]\n",
      " [7.16715377e-04 9.99283285e-01]\n",
      " [3.43247954e-05 9.99965675e-01]\n",
      " [8.34146562e-01 1.65853438e-01]\n",
      " [2.07857756e-03 9.97921422e-01]\n",
      " [1.37159472e-05 9.99986284e-01]\n",
      " [8.14445275e-07 9.99999186e-01]\n",
      " [2.08228792e-04 9.99791771e-01]\n",
      " [2.47038380e-06 9.99997530e-01]\n",
      " [6.37735135e-01 3.62264865e-01]\n",
      " [3.70726689e-03 9.96292733e-01]\n",
      " [7.99973081e-06 9.99992000e-01]\n",
      " [1.15713770e-01 8.84286230e-01]\n",
      " [2.90473899e-03 9.97095261e-01]\n",
      " [4.54922146e-05 9.99954508e-01]\n",
      " [2.21173915e-04 9.99778826e-01]\n",
      " [1.14078588e-04 9.99885921e-01]\n",
      " [2.04782944e-08 9.99999980e-01]\n",
      " [3.72478423e-05 9.99962752e-01]\n",
      " [9.43635232e-05 9.99905636e-01]\n",
      " [6.65690766e-01 3.34309234e-01]\n",
      " [1.33226763e-15 1.00000000e+00]\n",
      " [5.28901006e-05 9.99947110e-01]\n",
      " [2.60467346e-02 9.73953265e-01]\n",
      " [4.05029159e-03 9.95949708e-01]\n",
      " [2.77690715e-09 9.99999997e-01]\n",
      " [1.36282191e-02 9.86371781e-01]\n",
      " [5.46611487e-05 9.99945339e-01]\n",
      " [2.92752516e-04 9.99707247e-01]\n",
      " [1.29618390e-04 9.99870382e-01]\n",
      " [0.00000000e+00 1.00000000e+00]\n",
      " [1.79074000e-03 9.98209260e-01]\n",
      " [2.97385578e-04 9.99702614e-01]\n",
      " [7.03258160e-03 9.92967418e-01]\n",
      " [1.01243658e-01 8.98756342e-01]\n",
      " [1.58785412e-06 9.99998412e-01]\n",
      " [6.46578643e-06 9.99993534e-01]\n",
      " [1.46556207e-01 8.53443793e-01]\n",
      " [8.50120495e-04 9.99149880e-01]\n",
      " [2.01288938e-03 9.97987111e-01]\n",
      " [2.33753405e-07 9.99999766e-01]\n",
      " [2.29701691e-04 9.99770298e-01]\n",
      " [7.80636541e-02 9.21936346e-01]\n",
      " [4.54864889e-04 9.99545135e-01]\n",
      " [5.74684198e-01 4.25315802e-01]\n",
      " [8.09970936e-01 1.90029064e-01]\n",
      " [1.29225264e-02 9.87077474e-01]\n",
      " [7.10108335e-06 9.99992899e-01]\n",
      " [2.88638536e-02 9.71136146e-01]\n",
      " [2.76718719e-06 9.99997233e-01]\n",
      " [7.39902444e-04 9.99260098e-01]\n",
      " [9.45660311e-01 5.43396891e-02]\n",
      " [9.80609728e-01 1.93902715e-02]\n",
      " [8.28486628e-05 9.99917151e-01]\n",
      " [1.10761629e-05 9.99988924e-01]\n",
      " [4.21722551e-03 9.95782774e-01]\n",
      " [1.25580113e-05 9.99987442e-01]\n",
      " [2.63907064e-05 9.99973609e-01]\n",
      " [2.99419136e-04 9.99700581e-01]\n",
      " [2.48417701e-06 9.99997516e-01]\n",
      " [6.87541694e-03 9.93124583e-01]\n",
      " [8.42846152e-06 9.99991572e-01]\n",
      " [3.03132726e-05 9.99969687e-01]\n",
      " [3.31036431e-07 9.99999669e-01]\n",
      " [2.04613844e-04 9.99795386e-01]\n",
      " [7.99526695e-05 9.99920047e-01]\n",
      " [3.48198994e-03 9.96518010e-01]\n",
      " [2.64679275e-01 7.35320725e-01]\n",
      " [4.33506337e-02 9.56649366e-01]\n",
      " [9.33421152e-01 6.65788480e-02]\n",
      " [3.43948825e-06 9.99996561e-01]\n",
      " [2.65730721e-02 9.73426928e-01]\n",
      " [1.73650145e-03 9.98263499e-01]\n",
      " [1.77378576e-05 9.99982262e-01]\n",
      " [2.65404819e-07 9.99999735e-01]\n",
      " [1.22002249e-04 9.99877998e-01]\n",
      " [6.56599642e-07 9.99999343e-01]\n",
      " [3.52930329e-02 9.64706967e-01]\n",
      " [1.14509046e-01 8.85490954e-01]\n",
      " [1.71551209e-03 9.98284488e-01]\n",
      " [6.19723640e-06 9.99993803e-01]\n",
      " [5.39450815e-01 4.60549185e-01]\n",
      " [1.57918000e-02 9.84208200e-01]\n",
      " [4.09289380e-07 9.99999591e-01]\n",
      " [1.15558256e-02 9.88444174e-01]\n",
      " [1.00076457e-03 9.98999235e-01]\n",
      " [1.22204964e-02 9.87779504e-01]\n",
      " [1.30813582e-10 1.00000000e+00]\n",
      " [5.81033381e-05 9.99941897e-01]\n",
      " [1.11775655e-05 9.99988822e-01]\n",
      " [3.15158254e-04 9.99684842e-01]\n",
      " [2.47621157e-02 9.75237884e-01]\n",
      " [9.49233050e-01 5.07669496e-02]\n",
      " [1.66420417e-06 9.99998336e-01]\n",
      " [2.19760252e-05 9.99978024e-01]\n",
      " [2.68473597e-02 9.73152640e-01]\n",
      " [9.86832860e-01 1.31671399e-02]\n",
      " [4.76071638e-06 9.99995239e-01]\n",
      " [1.79756761e-06 9.99998202e-01]\n",
      " [2.21553194e-03 9.97784468e-01]\n",
      " [4.83509918e-06 9.99995165e-01]\n",
      " [2.88801609e-05 9.99971120e-01]\n",
      " [2.60170077e-07 9.99999740e-01]\n",
      " [1.23739686e-02 9.87626031e-01]\n",
      " [3.76058457e-02 9.62394154e-01]\n",
      " [1.48555039e-06 9.99998514e-01]\n",
      " [6.29544496e-03 9.93704555e-01]\n",
      " [1.91575276e-05 9.99980842e-01]\n",
      " [8.28309410e-09 9.99999992e-01]\n",
      " [8.15200782e-01 1.84799218e-01]\n",
      " [2.86056799e-07 9.99999714e-01]\n",
      " [1.88917762e-06 9.99998111e-01]\n",
      " [9.39017031e-07 9.99999061e-01]\n",
      " [2.04054411e-04 9.99795946e-01]\n",
      " [4.48043114e-04 9.99551957e-01]\n",
      " [1.12869683e-06 9.99998871e-01]\n",
      " [9.99908022e-01 9.19784067e-05]\n",
      " [1.93147498e-10 1.00000000e+00]\n",
      " [9.97599297e-01 2.40070268e-03]\n",
      " [2.91960392e-05 9.99970804e-01]\n",
      " [3.25103801e-03 9.96748962e-01]\n",
      " [8.24347419e-04 9.99175653e-01]\n",
      " [9.99819947e-01 1.80052792e-04]\n",
      " [8.96589007e-06 9.99991034e-01]\n",
      " [9.11888177e-04 9.99088112e-01]\n",
      " [3.05551517e-05 9.99969445e-01]\n",
      " [1.01772094e-04 9.99898228e-01]\n",
      " [2.74353759e-06 9.99997256e-01]\n",
      " [1.82605107e-02 9.81739489e-01]\n",
      " [1.53108188e-04 9.99846892e-01]\n",
      " [2.36425908e-01 7.63574092e-01]\n",
      " [3.14282650e-04 9.99685717e-01]\n",
      " [1.66011688e-04 9.99833988e-01]\n",
      " [3.73764523e-04 9.99626235e-01]\n",
      " [1.05271735e-03 9.98947283e-01]\n",
      " [1.30251733e-05 9.99986975e-01]\n",
      " [1.35040070e-04 9.99864960e-01]\n",
      " [2.98898938e-04 9.99701101e-01]\n",
      " [4.41702813e-03 9.95582972e-01]\n",
      " [1.34401378e-01 8.65598622e-01]\n",
      " [2.00957293e-01 7.99042707e-01]\n",
      " [6.67928914e-04 9.99332071e-01]\n",
      " [9.67914392e-05 9.99903209e-01]\n",
      " [2.93688636e-01 7.06311364e-01]\n",
      " [3.39597278e-05 9.99966040e-01]\n",
      " [4.96816852e-03 9.95031831e-01]\n",
      " [2.40419856e-05 9.99975958e-01]\n",
      " [1.67213392e-03 9.98327866e-01]\n",
      " [7.43234045e-02 9.25676596e-01]\n",
      " [5.81082262e-03 9.94189177e-01]\n",
      " [8.88178420e-16 1.00000000e+00]\n",
      " [2.10839218e-03 9.97891608e-01]\n",
      " [8.81064468e-01 1.18935532e-01]\n",
      " [1.86060432e-02 9.81393957e-01]\n",
      " [2.86381457e-02 9.71361854e-01]\n",
      " [1.18704785e-07 9.99999881e-01]\n",
      " [4.13838338e-03 9.95861617e-01]\n",
      " [7.59314103e-01 2.40685897e-01]\n",
      " [1.88508312e-03 9.98114917e-01]\n",
      " [4.44929538e-09 9.99999996e-01]\n",
      " [1.22480396e-01 8.77519604e-01]\n",
      " [3.80112839e-02 9.61988716e-01]\n",
      " [9.70808997e-07 9.99999029e-01]\n",
      " [4.21884749e-15 1.00000000e+00]\n",
      " [4.93492235e-06 9.99995065e-01]\n",
      " [2.84463538e-02 9.71553646e-01]\n",
      " [6.30999596e-01 3.69000404e-01]\n",
      " [1.98086657e-04 9.99801913e-01]\n",
      " [1.49190825e-05 9.99985081e-01]\n",
      " [3.73409807e-06 9.99996266e-01]\n",
      " [6.26254028e-03 9.93737460e-01]\n",
      " [2.83361718e-04 9.99716638e-01]\n",
      " [1.09852562e-03 9.98901474e-01]\n",
      " [5.83622350e-01 4.16377650e-01]\n",
      " [5.46735649e-08 9.99999945e-01]\n",
      " [1.00935626e-02 9.89906437e-01]\n",
      " [8.72312514e-01 1.27687486e-01]\n",
      " [7.18446635e-05 9.99928155e-01]\n",
      " [3.34308003e-04 9.99665692e-01]\n",
      " [5.91217291e-01 4.08782709e-01]\n",
      " [2.13990002e-05 9.99978601e-01]\n",
      " [6.40492100e-06 9.99993595e-01]\n",
      " [3.51219418e-03 9.96487806e-01]\n",
      " [3.95416856e-02 9.60458314e-01]\n",
      " [1.23134115e-04 9.99876866e-01]\n",
      " [2.02754439e-06 9.99997972e-01]\n",
      " [9.98527505e-02 9.00147249e-01]\n",
      " [2.15378231e-01 7.84621769e-01]\n",
      " [1.72371883e-03 9.98276281e-01]\n",
      " [4.05634892e-01 5.94365108e-01]\n",
      " [1.36555964e-01 8.63444036e-01]\n",
      " [2.22044605e-16 1.00000000e+00]\n",
      " [1.05853611e-03 9.98941464e-01]\n",
      " [1.04790177e-05 9.99989521e-01]\n",
      " [2.55022997e-03 9.97449770e-01]\n",
      " [9.09938791e-13 1.00000000e+00]\n",
      " [3.91294385e-05 9.99960871e-01]\n",
      " [4.61802811e-04 9.99538197e-01]\n",
      " [2.07309373e-04 9.99792691e-01]\n",
      " [7.29238928e-03 9.92707611e-01]\n",
      " [8.50501321e-04 9.99149499e-01]\n",
      " [7.99159389e-05 9.99920084e-01]\n",
      " [2.78569388e-04 9.99721431e-01]\n",
      " [1.11631906e-05 9.99988837e-01]\n",
      " [5.89242189e-04 9.99410758e-01]\n",
      " [1.10363283e-03 9.98896367e-01]\n",
      " [1.12209616e-05 9.99988779e-01]\n",
      " [4.29877454e-05 9.99957012e-01]\n",
      " [7.16093851e-13 1.00000000e+00]\n",
      " [9.99985698e-01 1.43022584e-05]\n",
      " [6.90975854e-09 9.99999993e-01]\n",
      " [7.73181752e-04 9.99226818e-01]\n",
      " [1.78836970e-02 9.82116303e-01]\n",
      " [2.41801074e-07 9.99999758e-01]\n",
      " [4.04907205e-03 9.95950928e-01]\n",
      " [3.11182880e-06 9.99996888e-01]\n",
      " [2.40751804e-08 9.99999976e-01]\n",
      " [6.70108908e-05 9.99932989e-01]\n",
      " [1.23826088e-02 9.87617391e-01]\n",
      " [2.55407883e-04 9.99744592e-01]\n",
      " [1.14846646e-01 8.85153354e-01]\n",
      " [9.98771282e-01 1.22871767e-03]\n",
      " [2.49732745e-05 9.99975027e-01]\n",
      " [1.55482240e-02 9.84451776e-01]\n",
      " [6.56910804e-04 9.99343089e-01]\n",
      " [7.11621865e-06 9.99992884e-01]\n",
      " [2.61937183e-03 9.97380628e-01]\n",
      " [1.59675384e-09 9.99999998e-01]\n",
      " [3.84007288e-02 9.61599271e-01]\n",
      " [5.10035959e-03 9.94899640e-01]\n",
      " [1.06305284e-03 9.98936947e-01]\n",
      " [1.21811862e-03 9.98781881e-01]\n",
      " [5.17522279e-06 9.99994825e-01]\n",
      " [3.45021492e-05 9.99965498e-01]\n",
      " [4.53491610e-04 9.99546508e-01]\n",
      " [7.93326709e-03 9.92066733e-01]\n",
      " [1.77420865e-01 8.22579135e-01]\n",
      " [2.06495995e-05 9.99979350e-01]\n",
      " [3.22716275e-03 9.96772837e-01]\n",
      " [1.98987550e-04 9.99801012e-01]\n",
      " [2.95159377e-03 9.97048406e-01]\n",
      " [3.83685825e-02 9.61631417e-01]\n",
      " [1.22707337e-06 9.99998773e-01]\n",
      " [1.96465254e-02 9.80353475e-01]\n",
      " [2.58597295e-01 7.41402705e-01]\n",
      " [1.17580785e-03 9.98824192e-01]\n",
      " [4.76458951e-03 9.95235410e-01]\n",
      " [1.34484040e-01 8.65515960e-01]\n",
      " [7.84523918e-07 9.99999215e-01]\n",
      " [6.48794350e-02 9.35120565e-01]\n",
      " [3.68961378e-01 6.31038622e-01]\n",
      " [7.94635985e-03 9.92053640e-01]\n",
      " [9.50781731e-04 9.99049218e-01]\n",
      " [3.90417382e-02 9.60958262e-01]\n",
      " [1.49641533e-01 8.50358467e-01]\n",
      " [1.82650270e-05 9.99981735e-01]\n",
      " [3.63189932e-03 9.96368101e-01]\n",
      " [7.23590400e-03 9.92764096e-01]\n",
      " [1.22030465e-05 9.99987797e-01]\n",
      " [6.19133960e-02 9.38086604e-01]\n",
      " [9.79180677e-06 9.99990208e-01]\n",
      " [6.23608280e-05 9.99937639e-01]\n",
      " [2.03602757e-02 9.79639724e-01]\n",
      " [1.56000402e-07 9.99999844e-01]\n",
      " [6.89362141e-02 9.31063786e-01]\n",
      " [1.06642243e-05 9.99989336e-01]\n",
      " [2.90484037e-05 9.99970952e-01]\n",
      " [4.45183990e-02 9.55481601e-01]]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "predict_proba()\n",
    "The returned estimates for all classes are ordered by the label of classes.\n",
    "\n",
    "Label 0 : Fail\n",
    "Label 1: Pass\n",
    "'''\n",
    "\n",
    "\n",
    "Probability=LogitReg.predict_proba(X_test)\n",
    "print(\"Shape: \",Probability.shape)\n",
    "print(\" Probability estimates: \",Probability)\n",
    "#print(\"Chance of Failure is \", 100*Probability[0,1], \"%\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicting the Test set Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=LogitReg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>301</th>\n",
       "      <th>302</th>\n",
       "      <th>303</th>\n",
       "      <th>304</th>\n",
       "      <th>305</th>\n",
       "      <th>306</th>\n",
       "      <th>307</th>\n",
       "      <th>308</th>\n",
       "      <th>309</th>\n",
       "      <th>310</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows √ó 311 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0    1    2    3    4    5    6    7    8    9    ...  301  302  303  304  \\\n",
       "0    1    1    1    1    1    1    1    1    1    1  ...    1    1    1    1   \n",
       "1    1    1    1    1    1    1    1    1    1    1  ...    1    1    1    1   \n",
       "\n",
       "   305  306  307  308  309  310  \n",
       "0    1    1    1    1    1    1  \n",
       "1    1    1    0    1    1    1  \n",
       "\n",
       "[2 rows x 311 columns]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare=[y_pred,y_test]\n",
    "pd.DataFrame(compare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(311,)\n",
      "(311,)\n"
     ]
    }
   ],
   "source": [
    "print(y_pred.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8906752411575563\n"
     ]
    }
   ],
   "source": [
    "accuracy = np.sum(y_pred==y_test)/len(y_test)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the performance using Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  4  14]\n",
      " [ 20 273]]\n",
      "Logistic regression model accuracy is= 89.06752411575563 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "print(\"Logistic regression model accuracy is=\", accuracy_score(y_test, y_pred)*100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-Sequential Model API:\n",
    "\n",
    "        It is referred to as ‚Äúsequential‚Äù because it involves defining a Sequential class and\n",
    "        adding layers to \n",
    "        the model one by one in a linear manner, from input to output.\n",
    "        \n",
    "        The sequential API is easy to use because you keep calling model.add() until you have\n",
    "        added all of your\n",
    "        layers.\n",
    "\n",
    "- Here, We are building a multilayer perceptron model(MLP), which is a standard fully connected neural network model.\n",
    "\n",
    "        It is comprised of layers of nodes where each node is connected to all outputs from the\n",
    "        previous layer \n",
    "        and the output of each node is connected to all inputs for nodes in the next layer.\n",
    "        \n",
    "        Multilayer Perceptron Model is appropriate for tabular data.\n",
    "        \n",
    "        There are three predictive modeling problems you may want to explore with an MLP; they\n",
    "        are binary classification, multiclass classification, and regression.\n",
    "  \n",
    "-It is a good practice to use ‚Äòrelu‚Äò activation with a ‚Äòhe_normal‚Äò weight initialization. This combination goes a long way to overcome the problem of vanishing gradients when training deep neural network models. \n",
    "        \n",
    "        (activation='relu', kernel_initializer='he_normal')\n",
    "        \n",
    "-For multiclass classification problems, the model must have one node for each class in the output layer and use the softmax activation function. The loss function is the ‚Äòsparse_categorical_crossentropy‚Äò, which is appropriate for integer encoded class labels (e.g. 0 for one class, 1 for the next class, etc.)\n",
    "\n",
    "\n",
    "-For regression problem involving predicting a single numerical value, the output layer must have a single node and uses the default or linear activation function (no activation function). The 'mean squared error' loss is minimized when fitting the model. \n",
    "\n",
    "Also , we cannot calculate  accuracy for regression problems. We only calculate accuracy for classification problems.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initializing the ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding the Input Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Desne: full connection\n",
    "\n",
    "model.add(tf.keras.layers.Dense(units=437,input_dim=437, activation='relu',kernel_initializer='he_normal'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding the Hidden Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(tf.keras.layers.Dense(units=218, activation='relu',kernel_initializer='he_normal'))\n",
    "model.add(tf.keras.layers.Dense(units=109, activation='relu',kernel_initializer='he_normal'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding the Output Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compiling the ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-Compiling the model requires that you first select a loss function that you want to optimize, such as mean squared error or cross-entropy or parse_categorical_crossentropy\n",
    "\n",
    "‚Äòbinary_crossentropy‚Äò for binary classification.\n",
    "‚Äòsparse_categorical_crossentropy‚Äò for multi-class classification.\n",
    "‚Äòmean squared error' for regression.\n",
    "-It also requires that you select an algorithm to perform the optimization procedure, typically stochastic gradient descent, or a modern variation, such as Adam.\n",
    "\n",
    "The optimizer can be specified as a string for a known optimizer class, e.g. ‚Äòsgd‚Äò for stochastic gradient descent, or you can configure an instance of an optimizer class and use that.\n",
    "\n",
    "opt = SGD(learning_rate=0.01, momentum=0.9)\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "model.compile(optimizer=opt, loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.051)\n",
    "\n",
    "model.compile(optimizer = optimizer, loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-Fitting the model requires that you first select the training configuration, such as the number of epochs (loops through the training dataset) and the batch size (number of samples/rows in an epoch used to\n",
    "estimate model error).\n",
    "\n",
    "-Training applies the chosen optimization algorithm to minimize the chosen loss function and updates\n",
    "the model using the backpropagation of error algorithm.\n",
    "\n",
    "-Fitting the model is the slow part of the whole process and can take seconds to hours to days, depending on the complexity of the model, the hardware you‚Äôre using, and the size of the training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-Generally speaking , training is stopped when the error made by the model falls to a low level or no longer improves, or a maximum number of epochs is performed. In this example, the training is stooped afte 100 epoch.\n",
    "\n",
    "-If the learning rate is too large, the algorithm will overshoot/skip the global cost minimum while if the learning rate is too small, the algorithm requires more epochs until convergence, which can make the learning slow, especially for large datasets.\n",
    "\n",
    "-While fitting the model, a progress bar will summarize the status of each epoch and the overall training process. This can be simplified to a simple report of model performance each epoch by setting the ‚Äúverbose‚Äù argument to 2. All output can be turned off during training by setting ‚Äúverbose‚Äù to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "50/50 [==============================] - 1s 4ms/step - loss: 2.9111 - accuracy: 0.8790 - val_loss: 0.3363 - val_accuracy: 0.9274\n",
      "Epoch 2/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.2791 - accuracy: 0.9284 - val_loss: 0.2631 - val_accuracy: 0.9274\n",
      "Epoch 3/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.2754 - accuracy: 0.9315 - val_loss: 0.2604 - val_accuracy: 0.9274\n",
      "Epoch 4/100\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.2478 - accuracy: 0.9325 - val_loss: 0.2605 - val_accuracy: 0.9274\n",
      "Epoch 5/100\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.2481 - accuracy: 0.9325 - val_loss: 0.2603 - val_accuracy: 0.9274\n",
      "Epoch 6/100\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.2474 - accuracy: 0.9325 - val_loss: 0.2603 - val_accuracy: 0.9274\n",
      "Epoch 7/100\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.2476 - accuracy: 0.9325 - val_loss: 0.2602 - val_accuracy: 0.9274\n",
      "Epoch 8/100\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.2477 - accuracy: 0.9325 - val_loss: 0.2601 - val_accuracy: 0.9274\n",
      "Epoch 9/100\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.2476 - accuracy: 0.9325 - val_loss: 0.2599 - val_accuracy: 0.9274\n",
      "Epoch 10/100\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.2472 - accuracy: 0.9325 - val_loss: 0.2601 - val_accuracy: 0.9274\n",
      "Epoch 11/100\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.2476 - accuracy: 0.9325 - val_loss: 0.2599 - val_accuracy: 0.9274\n",
      "Epoch 12/100\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.2471 - accuracy: 0.9325 - val_loss: 0.2605 - val_accuracy: 0.9274\n",
      "Epoch 13/100\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.2475 - accuracy: 0.9325 - val_loss: 0.2607 - val_accuracy: 0.9274\n",
      "Epoch 14/100\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.2472 - accuracy: 0.9325 - val_loss: 0.2605 - val_accuracy: 0.9274\n",
      "Epoch 15/100\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.2479 - accuracy: 0.9325 - val_loss: 0.2605 - val_accuracy: 0.9274\n",
      "Epoch 16/100\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.2475 - accuracy: 0.9325 - val_loss: 0.2606 - val_accuracy: 0.9274\n",
      "Epoch 17/100\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.2481 - accuracy: 0.9325 - val_loss: 0.2617 - val_accuracy: 0.9274\n",
      "Epoch 18/100\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.2473 - accuracy: 0.9325 - val_loss: 0.2608 - val_accuracy: 0.9274\n",
      "Epoch 19/100\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.2472 - accuracy: 0.9325 - val_loss: 0.2603 - val_accuracy: 0.9274\n",
      "Epoch 20/100\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.2474 - accuracy: 0.9325 - val_loss: 0.2604 - val_accuracy: 0.9274\n",
      "Epoch 21/100\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.2475 - accuracy: 0.9325 - val_loss: 0.2604 - val_accuracy: 0.9274\n",
      "Epoch 22/100\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.2470 - accuracy: 0.9325 - val_loss: 0.2604 - val_accuracy: 0.9274\n",
      "Epoch 23/100\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.2470 - accuracy: 0.9325 - val_loss: 0.2603 - val_accuracy: 0.9274\n",
      "Epoch 24/100\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.2470 - accuracy: 0.9325 - val_loss: 0.2604 - val_accuracy: 0.9274\n",
      "Epoch 25/100\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.2470 - accuracy: 0.9325 - val_loss: 0.2609 - val_accuracy: 0.9274\n",
      "Epoch 26/100\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.2468 - accuracy: 0.9325 - val_loss: 0.2606 - val_accuracy: 0.9274\n",
      "Epoch 27/100\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.2474 - accuracy: 0.9325 - val_loss: 0.2603 - val_accuracy: 0.9274\n",
      "Epoch 28/100\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.2465 - accuracy: 0.9325 - val_loss: 0.2614 - val_accuracy: 0.9274\n",
      "Epoch 29/100\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.2474 - accuracy: 0.9325 - val_loss: 0.2612 - val_accuracy: 0.9274\n",
      "Epoch 30/100\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.2467 - accuracy: 0.9325 - val_loss: 0.2606 - val_accuracy: 0.9274\n",
      "Epoch 31/100\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.2468 - accuracy: 0.9315 - val_loss: 0.2608 - val_accuracy: 0.9274\n",
      "Epoch 32/100\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.2472 - accuracy: 0.9325 - val_loss: 0.2603 - val_accuracy: 0.9274\n",
      "Epoch 33/100\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.2469 - accuracy: 0.9315 - val_loss: 0.2603 - val_accuracy: 0.9274\n",
      "Epoch 34/100\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.2463 - accuracy: 0.9325 - val_loss: 0.2603 - val_accuracy: 0.9274\n",
      "Epoch 35/100\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.2480 - accuracy: 0.9325 - val_loss: 0.2609 - val_accuracy: 0.9274\n",
      "Epoch 36/100\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.2478 - accuracy: 0.9325 - val_loss: 0.2603 - val_accuracy: 0.9274\n",
      "Epoch 37/100\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.2481 - accuracy: 0.9325 - val_loss: 0.2606 - val_accuracy: 0.9274\n",
      "Epoch 38/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.2489 - accuracy: 0.9325 - val_loss: 0.2609 - val_accuracy: 0.9274\n",
      "Epoch 39/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.2481 - accuracy: 0.9325 - val_loss: 0.2605 - val_accuracy: 0.9274\n",
      "Epoch 40/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.2485 - accuracy: 0.9325 - val_loss: 0.2608 - val_accuracy: 0.9274\n",
      "Epoch 41/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.2479 - accuracy: 0.9325 - val_loss: 0.2603 - val_accuracy: 0.9274\n",
      "Epoch 42/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.2479 - accuracy: 0.9325 - val_loss: 0.2605 - val_accuracy: 0.9274\n",
      "Epoch 43/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.2480 - accuracy: 0.9325 - val_loss: 0.2605 - val_accuracy: 0.9274\n",
      "Epoch 44/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.2477 - accuracy: 0.9325 - val_loss: 0.2604 - val_accuracy: 0.9274\n",
      "Epoch 45/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.2477 - accuracy: 0.9325 - val_loss: 0.2603 - val_accuracy: 0.9274\n",
      "Epoch 46/100\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.2479 - accuracy: 0.9325 - val_loss: 0.2618 - val_accuracy: 0.9274\n",
      "Epoch 47/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.2483 - accuracy: 0.9325 - val_loss: 0.2605 - val_accuracy: 0.9274\n",
      "Epoch 48/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.2478 - accuracy: 0.9325 - val_loss: 0.2603 - val_accuracy: 0.9274\n",
      "Epoch 49/100\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.2474 - accuracy: 0.9325 - val_loss: 0.2613 - val_accuracy: 0.9274\n",
      "Epoch 50/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.2484 - accuracy: 0.9325 - val_loss: 0.2604 - val_accuracy: 0.9274\n",
      "Epoch 51/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.2482 - accuracy: 0.9325 - val_loss: 0.2603 - val_accuracy: 0.9274\n",
      "Epoch 52/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.2483 - accuracy: 0.9325 - val_loss: 0.2608 - val_accuracy: 0.9274\n",
      "Epoch 53/100\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.2477 - accuracy: 0.9325 - val_loss: 0.2603 - val_accuracy: 0.9274\n",
      "Epoch 54/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.2484 - accuracy: 0.9325 - val_loss: 0.2611 - val_accuracy: 0.9274\n",
      "Epoch 55/100\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.2488 - accuracy: 0.9325 - val_loss: 0.2605 - val_accuracy: 0.9274\n",
      "Epoch 56/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.2486 - accuracy: 0.9325 - val_loss: 0.2613 - val_accuracy: 0.9274\n",
      "Epoch 57/100\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.2485 - accuracy: 0.9325 - val_loss: 0.2610 - val_accuracy: 0.9274\n",
      "Epoch 58/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.2481 - accuracy: 0.9325 - val_loss: 0.2608 - val_accuracy: 0.9274\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.2481 - accuracy: 0.9325 - val_loss: 0.2604 - val_accuracy: 0.9274\n",
      "Epoch 60/100\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.2478 - accuracy: 0.9325 - val_loss: 0.2604 - val_accuracy: 0.9274\n",
      "Epoch 61/100\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.2476 - accuracy: 0.9325 - val_loss: 0.2603 - val_accuracy: 0.9274\n",
      "Epoch 62/100\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.2478 - accuracy: 0.9325 - val_loss: 0.2604 - val_accuracy: 0.9274\n",
      "Epoch 63/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.2478 - accuracy: 0.9325 - val_loss: 0.2604 - val_accuracy: 0.9274\n",
      "Epoch 64/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.2484 - accuracy: 0.9325 - val_loss: 0.2603 - val_accuracy: 0.9274\n",
      "Epoch 65/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.2483 - accuracy: 0.9325 - val_loss: 0.2614 - val_accuracy: 0.9274\n",
      "Epoch 66/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.2495 - accuracy: 0.9325 - val_loss: 0.2617 - val_accuracy: 0.9274\n",
      "Epoch 67/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.2474 - accuracy: 0.9325 - val_loss: 0.2603 - val_accuracy: 0.9274\n",
      "Epoch 68/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.2478 - accuracy: 0.9325 - val_loss: 0.2606 - val_accuracy: 0.9274\n",
      "Epoch 69/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.2485 - accuracy: 0.9325 - val_loss: 0.2606 - val_accuracy: 0.9274\n",
      "Epoch 70/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.2487 - accuracy: 0.9325 - val_loss: 0.2604 - val_accuracy: 0.9274\n",
      "Epoch 71/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.2490 - accuracy: 0.9325 - val_loss: 0.2606 - val_accuracy: 0.9274\n",
      "Epoch 72/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.2480 - accuracy: 0.9325 - val_loss: 0.2603 - val_accuracy: 0.9274\n",
      "Epoch 73/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.2479 - accuracy: 0.9325 - val_loss: 0.2605 - val_accuracy: 0.9274\n",
      "Epoch 74/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.2490 - accuracy: 0.9325 - val_loss: 0.2607 - val_accuracy: 0.9274\n",
      "Epoch 75/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.2483 - accuracy: 0.9325 - val_loss: 0.2618 - val_accuracy: 0.9274\n",
      "Epoch 76/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.2486 - accuracy: 0.9325 - val_loss: 0.2612 - val_accuracy: 0.9274\n",
      "Epoch 77/100\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.2479 - accuracy: 0.9325 - val_loss: 0.2605 - val_accuracy: 0.9274\n",
      "Epoch 78/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.2481 - accuracy: 0.9325 - val_loss: 0.2622 - val_accuracy: 0.9274\n",
      "Epoch 79/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.2480 - accuracy: 0.9325 - val_loss: 0.2605 - val_accuracy: 0.9274\n",
      "Epoch 80/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.2480 - accuracy: 0.9325 - val_loss: 0.2607 - val_accuracy: 0.9274\n",
      "Epoch 81/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.2476 - accuracy: 0.9325 - val_loss: 0.2605 - val_accuracy: 0.9274\n",
      "Epoch 82/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.2482 - accuracy: 0.9325 - val_loss: 0.2614 - val_accuracy: 0.9274\n",
      "Epoch 83/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.2486 - accuracy: 0.9325 - val_loss: 0.2614 - val_accuracy: 0.9274\n",
      "Epoch 84/100\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.2479 - accuracy: 0.9325 - val_loss: 0.2604 - val_accuracy: 0.9274\n",
      "Epoch 85/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.2479 - accuracy: 0.9325 - val_loss: 0.2606 - val_accuracy: 0.9274\n",
      "Epoch 86/100\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.2487 - accuracy: 0.9325 - val_loss: 0.2603 - val_accuracy: 0.9274\n",
      "Epoch 87/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.2476 - accuracy: 0.9325 - val_loss: 0.2608 - val_accuracy: 0.9274\n",
      "Epoch 88/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.2491 - accuracy: 0.9325 - val_loss: 0.2603 - val_accuracy: 0.9274\n",
      "Epoch 89/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.2476 - accuracy: 0.9325 - val_loss: 0.2606 - val_accuracy: 0.9274\n",
      "Epoch 90/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.2484 - accuracy: 0.9325 - val_loss: 0.2612 - val_accuracy: 0.9274\n",
      "Epoch 91/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.2484 - accuracy: 0.9325 - val_loss: 0.2604 - val_accuracy: 0.9274\n",
      "Epoch 92/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.2479 - accuracy: 0.9325 - val_loss: 0.2606 - val_accuracy: 0.9274\n",
      "Epoch 93/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.2479 - accuracy: 0.9325 - val_loss: 0.2603 - val_accuracy: 0.9274\n",
      "Epoch 94/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.2478 - accuracy: 0.9325 - val_loss: 0.2607 - val_accuracy: 0.9274\n",
      "Epoch 95/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.2480 - accuracy: 0.9325 - val_loss: 0.2606 - val_accuracy: 0.9274\n",
      "Epoch 96/100\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.2482 - accuracy: 0.9325 - val_loss: 0.2603 - val_accuracy: 0.9274\n",
      "Epoch 97/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.2480 - accuracy: 0.9325 - val_loss: 0.2606 - val_accuracy: 0.9274\n",
      "Epoch 98/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.2478 - accuracy: 0.9325 - val_loss: 0.2605 - val_accuracy: 0.9274\n",
      "Epoch 99/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.2479 - accuracy: 0.9325 - val_loss: 0.2604 - val_accuracy: 0.9274\n",
      "Epoch 100/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.2482 - accuracy: 0.9325 - val_loss: 0.2612 - val_accuracy: 0.9274\n"
     ]
    }
   ],
   "source": [
    "#In our call to the fit function, we included reference to a validation dataset. \n",
    "#This validation dataset is a portion of the training set not used to fit the model, and is instead used\n",
    "# to evaluate the performance of the model during training (i.e. during each epoch)\n",
    "history = \\\n",
    "model.fit(X_train, y_train, \\\n",
    "             validation_split=0.2, batch_size=20, \\\n",
    "             epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_31 (Dense)             (None, 437)               191406    \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 218)               95484     \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 109)               23871     \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 1)                 110       \n",
      "=================================================================\n",
      "Total params: 310,871\n",
      "Trainable params: 310,871\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-From an API perspective, this involves calling a function with the holdout dataset and getting a loss and perhaps other metrics that can be reported.\n",
    "\n",
    "    model.evaluate(X_test,y_test)\n",
    "    \n",
    "-From an API perspective, you simply call a function to make a prediction of a class label, probability, or numerical value: whatever you designed your model to predict.\n",
    "\n",
    "\n",
    "-Learning curves are a plot of neural network model performance over time, such as calculated at the end of each training epoch.\n",
    "Plots of learning curves provide insight into the learning dynamics of the model, such as whether the model is learning well, whether it is underfitting the training dataset, or whether it is overfitting the training dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Prediction on the Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>427</th>\n",
       "      <th>428</th>\n",
       "      <th>429</th>\n",
       "      <th>430</th>\n",
       "      <th>431</th>\n",
       "      <th>432</th>\n",
       "      <th>433</th>\n",
       "      <th>434</th>\n",
       "      <th>435</th>\n",
       "      <th>436</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.236493</td>\n",
       "      <td>0.807648</td>\n",
       "      <td>0.749237</td>\n",
       "      <td>0.658846</td>\n",
       "      <td>0.295609</td>\n",
       "      <td>-0.244409</td>\n",
       "      <td>0.354172</td>\n",
       "      <td>0.006523</td>\n",
       "      <td>0.076290</td>\n",
       "      <td>0.510730</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.226606</td>\n",
       "      <td>-0.177645</td>\n",
       "      <td>-1.309625</td>\n",
       "      <td>0.118039</td>\n",
       "      <td>0.242327</td>\n",
       "      <td>0.119130</td>\n",
       "      <td>0.995903</td>\n",
       "      <td>-0.135310</td>\n",
       "      <td>-0.379026</td>\n",
       "      <td>-0.574642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.891407</td>\n",
       "      <td>0.478261</td>\n",
       "      <td>1.653159</td>\n",
       "      <td>1.548348</td>\n",
       "      <td>1.516175</td>\n",
       "      <td>-0.875219</td>\n",
       "      <td>0.028874</td>\n",
       "      <td>-1.622563</td>\n",
       "      <td>-0.691517</td>\n",
       "      <td>0.161239</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.212161</td>\n",
       "      <td>-0.130632</td>\n",
       "      <td>1.856825</td>\n",
       "      <td>-0.234572</td>\n",
       "      <td>-0.157099</td>\n",
       "      <td>-0.239381</td>\n",
       "      <td>0.433195</td>\n",
       "      <td>0.100836</td>\n",
       "      <td>-0.101434</td>\n",
       "      <td>-0.369428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.361772</td>\n",
       "      <td>-0.127876</td>\n",
       "      <td>-0.282960</td>\n",
       "      <td>0.128753</td>\n",
       "      <td>-0.884052</td>\n",
       "      <td>0.525423</td>\n",
       "      <td>0.516821</td>\n",
       "      <td>0.695906</td>\n",
       "      <td>-0.705106</td>\n",
       "      <td>-0.821704</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.218489</td>\n",
       "      <td>-0.497121</td>\n",
       "      <td>-0.223985</td>\n",
       "      <td>-0.112238</td>\n",
       "      <td>0.009329</td>\n",
       "      <td>-0.105173</td>\n",
       "      <td>0.531057</td>\n",
       "      <td>1.911292</td>\n",
       "      <td>2.154004</td>\n",
       "      <td>0.199102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.030172</td>\n",
       "      <td>-2.636249</td>\n",
       "      <td>-0.092781</td>\n",
       "      <td>0.097122</td>\n",
       "      <td>-0.773708</td>\n",
       "      <td>0.593752</td>\n",
       "      <td>0.078920</td>\n",
       "      <td>-0.230411</td>\n",
       "      <td>0.110263</td>\n",
       "      <td>-0.319311</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.257295</td>\n",
       "      <td>-0.015762</td>\n",
       "      <td>-0.465239</td>\n",
       "      <td>-0.040276</td>\n",
       "      <td>-0.023957</td>\n",
       "      <td>-0.036462</td>\n",
       "      <td>-1.703465</td>\n",
       "      <td>-0.562623</td>\n",
       "      <td>-0.552522</td>\n",
       "      <td>-1.045092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.993020</td>\n",
       "      <td>-0.398215</td>\n",
       "      <td>2.021213</td>\n",
       "      <td>0.482073</td>\n",
       "      <td>-0.302802</td>\n",
       "      <td>-0.656733</td>\n",
       "      <td>-0.296423</td>\n",
       "      <td>0.524564</td>\n",
       "      <td>-0.161526</td>\n",
       "      <td>1.286163</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.217901</td>\n",
       "      <td>-0.486937</td>\n",
       "      <td>-0.977902</td>\n",
       "      <td>0.168412</td>\n",
       "      <td>0.009329</td>\n",
       "      <td>0.165559</td>\n",
       "      <td>-0.162134</td>\n",
       "      <td>0.516903</td>\n",
       "      <td>0.696644</td>\n",
       "      <td>0.080709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>0.955419</td>\n",
       "      <td>0.648872</td>\n",
       "      <td>0.520647</td>\n",
       "      <td>1.489040</td>\n",
       "      <td>-0.292825</td>\n",
       "      <td>-0.288996</td>\n",
       "      <td>0.229057</td>\n",
       "      <td>0.517871</td>\n",
       "      <td>1.244987</td>\n",
       "      <td>1.209712</td>\n",
       "      <td>...</td>\n",
       "      <td>4.889454</td>\n",
       "      <td>6.385862</td>\n",
       "      <td>0.348991</td>\n",
       "      <td>-0.155415</td>\n",
       "      <td>-0.123813</td>\n",
       "      <td>-0.155091</td>\n",
       "      <td>-0.113203</td>\n",
       "      <td>0.449433</td>\n",
       "      <td>0.280255</td>\n",
       "      <td>0.017997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>0.293714</td>\n",
       "      <td>0.328976</td>\n",
       "      <td>2.475413</td>\n",
       "      <td>0.361360</td>\n",
       "      <td>0.097270</td>\n",
       "      <td>0.621816</td>\n",
       "      <td>-0.609210</td>\n",
       "      <td>-1.290588</td>\n",
       "      <td>-0.976896</td>\n",
       "      <td>0.510730</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.229299</td>\n",
       "      <td>-0.070735</td>\n",
       "      <td>-0.977902</td>\n",
       "      <td>-0.169807</td>\n",
       "      <td>-0.290240</td>\n",
       "      <td>-0.160514</td>\n",
       "      <td>0.531057</td>\n",
       "      <td>-0.472662</td>\n",
       "      <td>-0.274929</td>\n",
       "      <td>-0.587031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>0.262197</td>\n",
       "      <td>-1.017173</td>\n",
       "      <td>1.471928</td>\n",
       "      <td>1.880157</td>\n",
       "      <td>1.557280</td>\n",
       "      <td>-1.895193</td>\n",
       "      <td>-0.046194</td>\n",
       "      <td>-0.799319</td>\n",
       "      <td>1.645877</td>\n",
       "      <td>-0.384840</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.202623</td>\n",
       "      <td>-0.153217</td>\n",
       "      <td>0.107738</td>\n",
       "      <td>-0.277749</td>\n",
       "      <td>-0.256955</td>\n",
       "      <td>-0.264876</td>\n",
       "      <td>1.142696</td>\n",
       "      <td>0.179552</td>\n",
       "      <td>0.210857</td>\n",
       "      <td>-0.515544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>1.624189</td>\n",
       "      <td>0.120890</td>\n",
       "      <td>-0.722615</td>\n",
       "      <td>-0.921071</td>\n",
       "      <td>-1.190940</td>\n",
       "      <td>0.553842</td>\n",
       "      <td>-0.058706</td>\n",
       "      <td>-0.530259</td>\n",
       "      <td>-1.146765</td>\n",
       "      <td>-0.723410</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.213666</td>\n",
       "      <td>0.083856</td>\n",
       "      <td>-1.068372</td>\n",
       "      <td>-0.018688</td>\n",
       "      <td>0.175756</td>\n",
       "      <td>-0.010346</td>\n",
       "      <td>0.506591</td>\n",
       "      <td>-0.056595</td>\n",
       "      <td>0.002663</td>\n",
       "      <td>-0.445937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>2.303555</td>\n",
       "      <td>-2.208366</td>\n",
       "      <td>-1.008635</td>\n",
       "      <td>-0.685708</td>\n",
       "      <td>-0.038415</td>\n",
       "      <td>0.454746</td>\n",
       "      <td>-0.146286</td>\n",
       "      <td>0.169833</td>\n",
       "      <td>-0.650748</td>\n",
       "      <td>-0.526821</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.235058</td>\n",
       "      <td>-0.180227</td>\n",
       "      <td>0.680715</td>\n",
       "      <td>0.873635</td>\n",
       "      <td>0.575182</td>\n",
       "      <td>0.830972</td>\n",
       "      <td>0.115142</td>\n",
       "      <td>-0.371457</td>\n",
       "      <td>-0.483124</td>\n",
       "      <td>-0.446753</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>311 rows √ó 437 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6    \\\n",
       "0   -0.236493  0.807648  0.749237  0.658846  0.295609 -0.244409  0.354172   \n",
       "1   -0.891407  0.478261  1.653159  1.548348  1.516175 -0.875219  0.028874   \n",
       "2    0.361772 -0.127876 -0.282960  0.128753 -0.884052  0.525423  0.516821   \n",
       "3    0.030172 -2.636249 -0.092781  0.097122 -0.773708  0.593752  0.078920   \n",
       "4   -0.993020 -0.398215  2.021213  0.482073 -0.302802 -0.656733 -0.296423   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "306  0.955419  0.648872  0.520647  1.489040 -0.292825 -0.288996  0.229057   \n",
       "307  0.293714  0.328976  2.475413  0.361360  0.097270  0.621816 -0.609210   \n",
       "308  0.262197 -1.017173  1.471928  1.880157  1.557280 -1.895193 -0.046194   \n",
       "309  1.624189  0.120890 -0.722615 -0.921071 -1.190940  0.553842 -0.058706   \n",
       "310  2.303555 -2.208366 -1.008635 -0.685708 -0.038415  0.454746 -0.146286   \n",
       "\n",
       "          7         8         9    ...       427       428       429  \\\n",
       "0    0.006523  0.076290  0.510730  ... -0.226606 -0.177645 -1.309625   \n",
       "1   -1.622563 -0.691517  0.161239  ... -0.212161 -0.130632  1.856825   \n",
       "2    0.695906 -0.705106 -0.821704  ... -0.218489 -0.497121 -0.223985   \n",
       "3   -0.230411  0.110263 -0.319311  ... -0.257295 -0.015762 -0.465239   \n",
       "4    0.524564 -0.161526  1.286163  ... -0.217901 -0.486937 -0.977902   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "306  0.517871  1.244987  1.209712  ...  4.889454  6.385862  0.348991   \n",
       "307 -1.290588 -0.976896  0.510730  ... -0.229299 -0.070735 -0.977902   \n",
       "308 -0.799319  1.645877 -0.384840  ... -0.202623 -0.153217  0.107738   \n",
       "309 -0.530259 -1.146765 -0.723410  ... -0.213666  0.083856 -1.068372   \n",
       "310  0.169833 -0.650748 -0.526821  ... -0.235058 -0.180227  0.680715   \n",
       "\n",
       "          430       431       432       433       434       435       436  \n",
       "0    0.118039  0.242327  0.119130  0.995903 -0.135310 -0.379026 -0.574642  \n",
       "1   -0.234572 -0.157099 -0.239381  0.433195  0.100836 -0.101434 -0.369428  \n",
       "2   -0.112238  0.009329 -0.105173  0.531057  1.911292  2.154004  0.199102  \n",
       "3   -0.040276 -0.023957 -0.036462 -1.703465 -0.562623 -0.552522 -1.045092  \n",
       "4    0.168412  0.009329  0.165559 -0.162134  0.516903  0.696644  0.080709  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "306 -0.155415 -0.123813 -0.155091 -0.113203  0.449433  0.280255  0.017997  \n",
       "307 -0.169807 -0.290240 -0.160514  0.531057 -0.472662 -0.274929 -0.587031  \n",
       "308 -0.277749 -0.256955 -0.264876  1.142696  0.179552  0.210857 -0.515544  \n",
       "309 -0.018688  0.175756 -0.010346  0.506591 -0.056595  0.002663 -0.445937  \n",
       "310  0.873635  0.575182  0.830972  0.115142 -0.371457 -0.483124 -0.446753  \n",
       "\n",
       "[311 rows x 437 columns]"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This part passes? [ True]\n",
      "This part passes? [ True]\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "Pass = (y_pred > 0.5)\n",
    "# Just for quick test: \n",
    "print('This part passes?', Pass[1])\n",
    "print('This part passes?', Pass[27])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking the Confusion Matrix and Accuracy Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0  18]\n",
      " [  0 293]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9421221864951769"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "y_pred = (y_pred > 0.5) #Classification metrics can't handle a mix of binary and continuous targets, so we change the target (y_pred) to binary, True/False\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 1ms/step - loss: 0.2212 - accuracy: 0.9421\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.22121524810791016, 0.9421221613883972]"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Another quick way to check the loss and accuracy\n",
    "model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting the Learning Curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-Learning curves are a plot of neural network model performance over time, such as calculated at the end of each training epoch. So,  A learning curve is a plot of the loss on the training dataset and the validation dataset over the period of epochs.\n",
    "\n",
    "Plots of learning curves provide insight into the learning dynamics of the model, such as whether the model is learning well, whether it is underfitting the training dataset, or whether it is overfitting the training dataset.\n",
    "\n",
    "\n",
    "- Steps to create learning curves for your deep learning models.\n",
    "\n",
    "(1) you must update your call to the fit function to include reference to a validation dataset. This is a portion of the training set not used to fit the model, and is instead used to evaluate the performance of the model during training (i.e. during each epoch)\n",
    "\n",
    "(2) You can split the data manually and specify the validation_data argument, or you can use the validation_split argument and specify a percentage split of the training dataset and let the API perform the split for you. The latter is simpler for now.\n",
    "\n",
    "(3) The fit function will return a history object that contains a trace of performance metrics recorded at the end of each training epoch. This includes the chosen loss function and each configured metric, such as accuracy, and each loss and metric is calculated for the training and validation datasets.\n",
    "\n",
    "(4) A learning curve is a plot of the loss on the training dataset and the validation dataset. We can create this plot from the history object using the Matplotlib library.\n",
    "\n",
    "\n",
    "-To interpret a l learing curve, see this link:https://machinelearningmastery.com/learning-curves-for-diagnosing-machine-learning-model-performance/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x504 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Learning Curve')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(['Train','Val'], loc='upper left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation of the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------\n",
    "ANN MODEL\n",
    "\n",
    "---------------\n",
    "the ANN model was overfitting the data. The overfitting was Unfortunately obtained by zeroing the dropout rate and running numerous epochs through the training dataset,with a sufficiently small learning rate.\n",
    "\n",
    "After using a larger learning rate, it was noticed that the training and validation losses were both contiuning to decrease. As a result, I obtained the above learning curve showing a good fit.\n",
    "\n",
    "\n",
    "The plot of learning curves shows a good fit since:\n",
    "\n",
    "-The plot of training loss decreases to a point of stability.\n",
    "-The plot of validation loss decreases to a point of stability and has a very small gap with the training loss.\n",
    "\n",
    "Continued training of a good fit model will likely lead to an overfit. Therefore, the current ANN model with a 94.2% accuracy rate is otptimized.\n",
    "\n",
    "---------------------------------------------\n",
    "ANN MODEL VS LOGISTIC REGRESSION\n",
    "\n",
    "---------------------------------------------\n",
    "When comparing the ANN model with the logsitic regression model, The ANN model is chosen for its higher accuracy of 94.2% vs 89% for the logisitc regression.\n",
    "\n",
    "However, in case that we do not to use a model that is computionally-intnesive, as in the case of ANN, the logsisitc regression model is preffered since its accuracy is very good and closer to the ANN model. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REFERENCES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "[1] https://machinelearningmastery.com/tensorflow-tutorial-deep-learning-with-tf-keras/\n",
    "\n",
    "[2] https://machinelearningmastery.com/learning-curves-for-diagnosing-machine-learning-model-performance/"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "data_preprocessing_tools.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "392.017px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 357.5,
   "position": {
    "height": "379.5px",
    "left": "1161px",
    "right": "20px",
    "top": "122px",
    "width": "319px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
